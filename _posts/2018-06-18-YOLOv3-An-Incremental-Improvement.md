---
layout: post
title:  "YOLOv3: An Incremental Improvement"
categories: ObjectDetection
tags:  ObjectDetection YOLO
author: CQ
---

* content
{:toc}

**Intro:** arXiv 2018.04  

**Link:** [https://arxiv.org/abs/1804.02767](https://arxiv.org/abs/1804.02767)  

**Code:**  
[https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/)  
[https://github.com/pjreddie/darknet](https://github.com/pjreddie/darknet)



## 摘要：

　　我们发布对YOLO的一些更新。我们使用一系列小改变让它变得更好。我们也将这个新网络训练的很好。它比上次稍大一点，但更准确。不用担心，它仍然足够快。在320×320上YOLOv3运行22ms，mAP得分28.2，与SSD同样准确但快3倍。当我们观察原来的0.5　IOU mAP检测标准，YOLOv3相当好。它达到57.9 AP50得分，在TitanX上运行速度51ms，相比Retinanet的57.5 AP50得分运行速度198ms，想死的性能但快3.8倍。与以前一样，代码链接: [https://pjreddie.com/yolo/](https://pjreddie.com/yolo/)。  

## 1.简介

　　有时你只是想问一句，是不？今年我没有做很多研究。花了很多时间在Twitter上。跟GAN打了会交道。从去年开始我只剩下很少的时间，我设法对YOLO做一些改进。但，实话实说，没有什么特别有趣的，只有一串小改变让它变得更好。我也在别人的研究上帮了点忙。  
　　事实上，就是这个把我们带到这里的。我们有完稿截止期，需要引用一些随机的我给YOLO做出的更新，但我们还没有更新的来源。所以，为技术报告做好准备吧。  
　　技术报告很好的一点是它不需要简介，我们都知道为什么来这儿。所以这简介的结尾将标识文章剩余的部分。首先，我们告诉你对YOLOv3都做了那些处理。然后，我们告诉你我们是怎么做的。我们也会告诉你那些我们尝试了但不起作用的东西。最后，我们会思考上述这些都意味着什么。  

![](/assets/YOLOv3_An_Incremental_Improvement/Figure_1.png)
图1. 我们采用来自Focal Loss文章的这张图。YOLOv3运行显著快于其他检测方法且有有竞争力的性能。时间来自M40或者TitanX，他们基本是相同的GPU。  

## 2. 处理方法

　　这就是对YOLOv3的处理：我们主要采纳其他人的好主意。我们也训练一个新的比其他更好的分类网络。我们将从零开始带你穿越整个系统，使你能全部理解。  

### 2.1 边界框预测

　　沿袭YOLO9000，我们的系统使用anchor boxes的维度聚类预测边界框。网络预测对每个边界框4个坐标，$t_x$,$t_y$,$t_w$,$t_h$。如果格子从图像的左上角平移$(c_x,c_y)$且预设边框的宽度和高度为$p_w,ph$，那么预测符合：  

$$
b_x=\sigma(t_x)+c_x \\
b_y=\sigma(t_y)+c_y \\
b_w=p_wexp(t_w)
b_h=p_hexp(t_h)
$$

　　训练过程中我们使用平方差损失之和。如果对某些坐标预测的真值为$\widehat{t^*}$，梯度就是真值(由真值边界框计算)减去预测值：$\widehat{t^*}-t^*$。这个真值可以通过上述公式取逆很容易计算。  

　　在YOLOv3中预测使用logistic　regression预测每个边界框的目标得分。如果一个预设边界框比其他所有边界框覆盖真值目标都多，那得分应该为1。如果预设边界框不是最好的，但确实以超过某个临界值覆盖一个真值目标，我们忽略这个预测，沿袭[17]。我们使用临界值0.5。不像[17]，我们的系统对于一个真值目标只指定一个预设边界框。如果一个预设边界框没有指定真值目标，，它不会对坐标或类别预测引发损失，只对目标。  

![](/assets/YOLOv3_An_Incremental_Improvement/Figure_2.png)
图2. 预设维度和位置预测的边界框。我们以从聚类中心的平移预测边界框的宽度和高度。我们使用sigmoid函数预测边界框中心相对于filter　application位置的坐标。这张图公然抄袭自[15]。  

### 2.2 类别预测

　　每个框使用多标签分类预测边界框可能包括的类别。我们不适用softmax因为我们发现它对于好性能不是必要的，相反我们简单的使用独立logistic分类器。训练过程中我们使用二值交叉熵损失做分类预测。  

　　当我们移到更复杂的领域如Open Images Dataset时，这个公式很有帮助。这个数据集中有很多互相覆盖的标签(如女人和人)。使用softmax就强加了每个框有精确的一个类别的假设，通常不是这样。多标签方法建模数据更好。  

### 2.3 跨尺度预测

　　YOLOv3在3个不同尺寸预测边界框。我们的系统使用与feature pyramid networks类似的概念从这些尺寸提取特征。从基准特征提取器我们增加了一些卷积层。最后一层预测一个3-d张量，编码了边界框，目标和类别预测。在我们的COCO实验中，我们在每个尺寸预测3个框，所以对于4个边界框平移1个目标预测和80个类别预测，张量是$N\times N\times [3\times (4+1+80)]$。  

　　然后我们用前面两层的feature map，进行2倍升采样。我们也拿网络更早期的feature map，用串联的方式将其与升采样特征合并。这个方法允许我们从更早的feature map中得到更有意义的语义信息。然后我们又增加了一些卷积层处理这些组合feature map，最终预测一个类似的张量，尽管现在两倍尺寸。  

　　我们对最终尺寸再执行一次相同的过程。因此我们对于第三个尺寸的预测获益于所有前面的计算，以及网络内早期纹理精细的特征。  

　　我们仍然使用k-means聚类在决定预设边界框。我们只任意的稍微分了9聚类和3个尺度，然后跨尺度分离聚类。在COCO数据集这9个聚类是：(10×13),(16×30),(33×23),(30×61),(62×45),(59×119),(116×90),(156×198),(373×326)。  

### 2.4 特征提取器

　　我们使用一个新的网络执行特征提取。我们的新网络是一个杂交网络，在YOLOv2中使用的Darknet-19和新式的残差网络材料。我们的网络使用连续的3×3和1×1卷积层，但现在也有一些跨越链接，并且明显更大。它有53卷积层，所以我们称他为Darknet-53。  

　　这个新网络比Darknet-19强力很多，但仍然比ResNet-101或ResNet-152高效。这是一些ImageNet结果。  

　　每个网络以相同的设置进行训练，在256×256单一剪切准确度上测试。运行时间在TitanX上测量，尺寸256×256。Darknet-53与最新水平分类器表现相当，但是有更少的浮点数操作和更快的速度。Darknet-53与ResNet-152有相同的表现且快2倍。  

　　Darknet-53也获得最高的测量的每秒浮点数操作。这意味着网络结构更好的利用GPU，让它更高效的评测，因此更快。这主要因为ResNets有太多层且不够高效。  

![](/assets/YOLOv3_An_Incremental_Improvement/Table_1.png)
表1. Darknet-53  
![](/assets/YOLOv3_An_Incremental_Improvement/Table_2.png)
表2. 主干对比。不同网络的准确度、百万操作、每秒百万浮点数操作和FPS。  

### 2.5　训练

　　我们仍然在所有图像上训练，没有hard negative mining或者类似的东西。我们使用多尺度训练，很多数据扩展，batch normalization，所有标准手段。我们使用Darknet神经网络框架进行训练和测试。  

## 3. 我们怎么做的

　　YOLOv3相当棒！如表3所见。在COCO在线average mean AP度量标准下，它与SSD的变体相当但快3倍。它在此标准下仍然落后于其他模型如RetinaNet相当的距离。  

　　然而，当我们看'老'的mAP在IOU=50(或者表中的AP50)的检测标准，YOLOv3相当强壮。它几乎与RetinaNet相当，大幅超过SSD变体。这显示YOLOv3是很强的检测器，擅长对目标产生相当好的边界框。然而，随着IOU临界值增加的性能明显下降，说明YOLOv3与得到和目标对齐的完美边界框而斗争。  

　　过去YOLO与小目标斗争。然而，现在我们在那个趋势上看到了逆转。有了新的多尺度预测，我们看到YOLOv3有相当高的APs性能。然而，它在中尺寸目标和大尺寸目标上有比较低的性能。为了达到这点需要更多的研究。  

　　当我们在AP50标准(如图5)上画出准确度vs速度曲线时，我们看到YOLOv3比其他检测系统有显著的效益。  

## 4. 我们尝试了但无效的事

　　当我们修改YOLOv3时我们尝试了很多东西。很多都无效。这里是我们要记住的那些。  

　　Anchor box x y 平移预测。我们尝试使用通常anchor box预测机制，即使用线性激活以边界框宽度或高度的倍数预测x y平移。我们发现这种形式降低模型稳定性且效果不好。  

　　线性x y预测而不是logistic。我们尝试使用线性激活直接预测x y平移而不是logistic激活。这导致几点mAP降低。  

　　Focal Loss。我们尝试使用Focal Loss。它降低了约2分mAP。YOLOv3对于Focal Loss尝试解决的问题已经强壮了，因为它有分离的目标预测和条件类别预测。因此对大多数样本，没有类别预测产生的损失？或者别的？我们不完全明白。  

![](/assets/YOLOv3_An_Incremental_Improvement/Table_3.png)
表3. 我很严肃的从[9]中偷来所有这些表格，从零开始制作花太长时间了。好了，YOLOv3运行不错。记住Retinanet处理一张图像有3.8倍长的时间。YOLOv3比SSD变体好得多，在AP50标准上与最新水平模型有的比较。  
![](/assets/YOLOv3_An_Incremental_Improvement/Figure_3.png)
图3. 再一次采用自[9]，这次展示mAP在0.5标准上的速度/准确度权衡。你可以说YOLOv3很好，因为它很高且远在左边。你可以引用自己的文章吗？猜猜谁试了试，这个家伙[16]。噢，我忘了，我们也修复了一个YOLOv2中数据载入的bug，这帮助提高差不多2mAP。偷偷在这里说这个，为了不抛弃布局。  

　　双IOU临界值和真值设定。Faster R-CNN在训练过程中使用两个IOU临界值。如果预测以0.7覆盖真值它就是正样本，0.3~0.7样本被忽略，对徐偶有真值目标小于0.3他是负样本。我们尝试了类似的策略但没有得到好结果。  

　　我们相当喜欢现在的形式，它看起来至少在局部最优。很可能这些技术中的一部分能最终产生好结果，也许他们只需要一些微调来稳定训练过程。  

## 5. 这些意味着什么

　　YOLOv3是一个很好的检测器。快速而准确。它在COCO average AP　0.5到0.95不够好，但在IOU为0.5的旧检测标准非常好。  

　　为什么我们改变标准？原始COCO文章正好有这样模糊的一句：一旦评测服务器完成，一个评价标准的全面讨论将增加上。Russakovsky et al指出这一点，人类很难区分IOU从0.3到0.5。训练人类视觉检查IOU为0.3的边界框并将其与IOU为0.5的进行区分，这相当困难。如果人类很难给出差别，那又有什么意义？  


　　但也许一个更好的问题是：现在我们有了检测器，我们打算用它们做什么？很多人在Google和Facebook做这项研究。我猜至少我们知道这项技术很有帮助，而且肯定不会用来收集你的个人信息然后卖给...等等，你在说这正是它的用处？噢...  

　　好吧，大量投资视觉研究的人是军队，他们绝不会做任何像用新技术杀人这类可怕的事，等等...  

　　我很希望大多数使用计算机视觉的人都用它做令人高兴的好的事情，像在国家公园统计斑马数量，当他们的猫在房子周围散步时或者跟踪它们。但计算机视觉已经被放进有问题的用途，最为研究者我们至少有责任考虑我们的工作可能造成的伤害，考虑减轻伤害的方法。我们欠这个世界太多。  

　　最后，不要@我(因为我退出Twitter了)。  

![](/assets/YOLOv3_An_Incremental_Improvement/Figure_4.png)
图4. 零轴图也许更理智上诚实...而且我们仍然可以用变量钉牢让我们看起来更好。  

## 参考文献：

## 辩驳

　　我们感谢Reddit留言的人，实验室人员，邮件我的人，大声感谢他们的可爱热心的言语。如果你，像我，正在评审ICCV，我们知道你可能有其他37篇文章要读，并且直到最后一周拒掉它们，领域里有一些日程发邮件给你说你应该完成这些评审，除非完全不明白它们在说什么，就像来自未来的文章？不管怎样，这些文章不会及时变成它将要成为的样子，除非过去的你自己将在过去做完工作，但只是向前一点，不像自始至终直到现在再向前。如果你叫嚣这个我是不会知道的。只是提一句。  

　　评审人2 Dan Grossman(哈哈哈无视谁做的吧)坚持认为，我在这里指出，我的图示只有两个原始图。Dan你完全正确，这是因为这看起来比对自己承认我们在这儿只是为了2-3% mAP而战斗更好。但是有需要的图示。我也扔进一个FPS图示，因为当我们画FPS时看起来超级好。  

　　评审人4　JudasAdventus在Reddit上写"容易阅读，但对MSCOCO标准的议论似乎有点不牢固"。好吧，我总知道你会是那个激发我的人，Judas。你知道　当你在一个项目上工作，他只是有结果出来，所以你需要找出一些方法评价你做的东西实际上有多好？我尽量做这件事，然后在COCO标准上被打击了一下。但现在我已经标出这个我可能会死在上面的点。  

　　看就是这点，mAP已经有点坏掉了，所以一个对它的更新可能解决一些它的问题或者至少证明为什么更新版本在某方面更好。这个我争论很大的事是缺少证明。对于PASCAL VOC，IOU临界值被"慎重的设置低，为了统计真值数据边界框中的不精确"。COCO有比VOC更好的标签吗？这肯定是对的，因为COCO有分割掩膜，也许标签更真实，因此我们不必担心不精确。有好的理由认为更精确的边界框比更好分类更重要吗？一个错误分类样本比一个边界框稍微平移更明显。  

　　mAP已经完蛋了，因为所有它关心的是每个类别的排列顺序。例如，如果你的测试集只有这两张照片，根据mAP，产生这些结果的两个检测器同样好：  

![](/assets/YOLOv3_An_Incremental_Improvement/Figure_5.png)
图5. 根据这两张图上的mAP，这两个假设检测器完美。他们同样完美。完全相同。  

　　现在，这显然是一个mAP问题的过大夸张，但我猜我的新观点是，现实世界中人们关心的和我们现在的标准有这么明显的矛盾，我认为如果打算继续提出新标准，我们应该关注这些，矛盾。同样，已经是mean average precision，我们甚至怎么称呼COCO标准，average mean average precision？  

　　这里有个建议，人们真正关心的是，有一个图像和一个检测器，检测器能在图像中寻找和分类目标做的多好。抛弃每类AP而值做一个global average percision如何？或者每个图像做AP计算然后在此之上做平均？  

　　不管怎样，边界框很傻，我可能是一个相信mask的人，除了我不能让YOLO学习他们。  
