---
layout: post
title:  "Hindsight Experience Replay"
categories: ReinforcementLearning
tags:  ReinforcementLearning, HER
author: CQ
---

* content
{:toc}

**Intro:** arxiv 1707  

**Link:** [https://arxiv.org/abs/1708.05144](https://arxiv.org/abs/1707.01495)  

**Code:** [https://github.com/openai/baselines](https://github.com/openai/baselines)  




## 摘要：

　　处理稀疏回报是强化学习最大的挑战之一。我们提出一个称为‘事后经验回顾’的全新技术，它允许从稀疏及二值奖励中高效抽样学习，因此避免了对复杂奖励处理的需求。它可以与任何离线策略强化学习算法结合，也可被视为是一种隐式过程。
　　我们在用机械臂操作物体的任务中证明了我们的方法。特别的，我们在三个不同任务上进行实验：推动，滑动，拿取-放置，每个情况下只使用二值奖励表示任务是否完成。我们的简化模型实验显示‘事后经验回顾’是一个关键组分，使这些挑战性环境中的训练成为可能。我们展示我们的在物理模拟器上训练的策略可以部署在真实物理机械臂上，并成功完成相应任务。实验视频链接: https://goo.gl/SMrQnI.  

## 1.简介

　　在学习序列决策问题的策略中，结合神经网络的强化学习最近导致一个大范围的成功。这包括模拟环境，如Atari游戏(Mnih等，2015)，和在围棋上击败最好的人类选手(Silver等, 2016)，以及机器人任务如直升机控制(Ng等，2006)，打击棒球(Peter和Schaal，2008)，拧紧瓶盖(Levine等，2015)或者开门(Chebotar等，2016)。  

　　然而，一个普遍的挑战是需要制造一个回报函数，不仅反应当前的任务，而且需要要仔细的修正(Ng等，1999)来指导策略优化。例如，Popov等人(2017)使用损失函数包含五个相关的复杂的项，需要仔细权重以便训练一个策略实现将一个块体放在另一个的顶上。制造损失函数的必要性限制了现实世界中强化学习的应用，因为它即需要强化学习专精，也需要专业领域知识。更进一步，它在我们不知道合理的行为是什么样的情况下，并不适用。因此开发可以从无修正回报信号中学习的算法，如表示任务成功完成的二值信号，在极大程度上是经验相关的。  

　　人类有一项能力是从不需要的输出中学到的和从需要的输出中学到的几乎同样多，不像现有的模型无关强化学习算法。想想你在学习如何玩儿曲棍球，尝试将冰球击入网中。你击求但球落在网右边没能入网。标准强化学习算法在这种情况下得出的结论将是执行的动作序列没能导致成功的打击，很少量(如果有)将被学习。然而可能得出另一个结论，就是如果网再放置更右边一点，这个动作序列将会成功。  

　　本文中，我们引入一个技术叫做‘事后经验回顾’(HER)，允许算法执行这类推断且能与任意离线策略强化学习算法组合。它可用于任何多目标获取的情况，如达到系统的每个状态可以视作单独的目标。这种设置先，HER不仅改进了抽样效率，更重要的，即便回报信号是稀疏且二值的，它也使得学习成为可能。我们的方法基于训练整体策略(Schaul等，2015)，将当前状态和目标状态作为输入。HER背后关键的想法是以不同的目标回放每个计算轮，而不是agent试图达到的目标，如在本计算轮中达到的目标之一。  

## 2.背景

　　本节中，我们介绍本文中用到的强化学习形式，以及我们实验中的强化学习算法。  

### 2.1 强化学习

　　我们考虑标准强化学习形式，包含一个agent与环境交互作用。为简化解释，我们假设环境可以全面观察。环境被一盒状态集合$S$，一个动作集合$A$，一个初始状态分布$P(s_0)$，一个奖励函数$r:S \times A \rightarrow R$，状态转移概率$p(s_{t+1} \mid s_t,a_t)$和折扣因子$\gamma \in [0,1]$描述。  

　　一个精确策略是状态到动作的映射 $\pi : S \rightarrow A$。每个计算轮开始于抽样一个初始状态$s_0$。在每个时间步t，agent基于当前状态产生一个动作：$a_t=\pi(s_t)$。然后得到奖励$r_t=r(s_t,a_t)$，环境的新状态从分布$p(\cdot \mid s_t,a_t)$中抽样获得。将来奖励折减求和称为回报：$R_t=\sum_{i=t}^{\infty}{\gamma^{i-t}r_i}$。agent的目标是最大化回报期望$E_{s_0}[R_0 \mid s_0]$。Q函数或者动作值函数定位为$Q^{\pi}(s_t,a_t)=E[R_t \mid s_t,a_t]$。  

　　令$\pi^*$表示最佳策略，即任何策略$\pi^*$使得$Q^{\pi} \geq Q^{\pi}(s,a)$，对每个$s \in S$,$ a \in A$及任意策略$\pi$。  
所有最优策略有相同的Q函数，称为最优Q函数，表示为Q*。很容易显示，它满足如下的Bellman方程  

$$
Q^*(s,a)=E_{s'~p(\cdot \mid s,a)}[r(s,a)+\gamma \displaystyle \max_{a' \in A} Q^*(s',a')]
$$

### 2.2 Deep Q-Networks(DQN)

　　Deep Q-Networks(DQN)(Minh等，2015)是一个离散动作空间中的模型无关强化学习算法。这里我们非正式的介绍它一下，更多细节见Minh等人(2015)的文章。在DQN中我们维护一个神经网络Q近似Q*。关于Q的贪婪策略定义为$\pi_Q(s) = argmax_{a \in A}Q(s,a)$。关于Q的$\epsilon$贪婪策略是以概率$\epsilon$执行随机动作(从A中均匀抽样)以及以概率$1-\epsilon$执行$\pi_Q(s)$动作。  

　　在训练过程中，我们使用当前动作值函数Q的近似函数的$\epsilon$贪婪策略生成计算轮。训练过程中累计的转移元组$(s_t, a_t, r_t, s_{t+1})$被存储在称为回放缓存(replay buffer)的地方。新计算轮的生成与神经网络训练交互进行。网络使用mini-batch梯度下降进行训练，损失函数L促使近似Q函数满足Bellman方程$L=E(Q(s_t, a_t)-y_t)^2$，此处$y_t=r_t+\gamma \displaystyle \max_{a' \in A}Q(s_{t+1},a')$，元组$(s_t, a_t, r_t, s_{t+1})$从回放缓存中抽样。  

　　为了使这个优化过程更稳定，目标$y_t$通常使用单独的目标网络进行计算，比主网络改变速度更缓慢。一个通常的实现是周期的设置目标网络权重为当前主网络权重(如Mnih等人(2015))或者使用主网络的Polyak平均(Pyloak和Juditsky，1992)版(Lillicrap等人，2015)。  

### 2.3 Deep Deterministic Policy Gradients(DDPG)

　　Deep Deterministic Policy Gradients(DDPG)(Lillicrap等人，2015)是一个连续动作空间的模型无关强化学习算法。这里我们非正式的介绍它，更多细节见Lillicrap等人(2015)的文章。在DDPG中我们维护两个神经网络，目标策略(也称为actor)$\pi : S \rightarrow A$和一个动作值函数近似器(critic)$Q:S \times A \rightarrow R$。critic的工作是近似actor的动作值函数Q。  

　　计算轮使用行为策略生成，一个噪声版本的目标策略，如$\pi_b(s) = \pi(s) + N(0,1)$。critic以DQN中Q函数类似的方式进行训练，但目标$y_t$使用actor输出的动作进行计算，如$y_t=r_t+ \gamma Q(s_{t+1},\pi(s_{t+1}))$。actor以mini-batch梯度下降进行训练，损失函数$L_a=-E_sQ(s,\pi(s))$，s从回放缓存中抽样。La对actor参数的梯度可以通过反向传播critic和actor联合的网络进行计算。  

### 2.4 全局值函数近似(Universal Value Function Approximators, UVFA)

　　Universal Value Function Approximators(UVFA)(Schaul等人，2015)是DQN对我们需要达到多于一个目标的扩展。令G为可能目标空间。每个目标$g \in G$相应某个回报函数$r_g:S \times A \rightarrow R$。每个计算轮开始于从某个抽样$p(s_0,g)$中抽样一个状态-目标对。目标在整个计算轮中固定不变。  

　　每个计算步agent不仅使用当前状态，还使用当前目标$\pi:S \times G \rightarrow A$作为输入，得到奖励$r_t=r_g(s_t,a_t)$。Q函数现在不仅依赖于状态-动作对，而且依赖于目标$Q^\pi(s_t,a_t,g)=E[R_t \mid s_t,a_t,g]$。Schaul等人(2015a)展示在这种设置下，使用直接自Bellman方程的导出(就如同DQN)训练一个Q函数近似器是可能的，从此导出的贪婪策略可以扩展到先前未见到的状态-动作对。这个对DDPG扩展的方法是直接了当的。  

## 3. 事后经验回放

## 3.1 一个启发性的例子

　　考虑一个位翻转环境，状态空间为$S=\lbrace 0,1 \rbrace ^n$以及动作空间$A=\lbrace 0,1,...,n-1 \rbrace$对某个整数n，执行i个动作翻转状态的第i位。对每个计算轮，我们均匀抽样一个初始状态以及目标状态，只要不是目标状态策略就得到-1奖励，即$r_g(s,a)=-[s \neq g]$。  

　　标准强化学习算法在这个环境下对于n>40必然失效，因为他们除-1不会获得任何奖励。注意到使用增加探索的技术，基于计数的探索，或自荐式DQN在这里并没有帮助，因为真正的问题不是观察到状态的多样化的缺失，何况探索如此大状态空间是不现实的。这个问题的标准解法是使用修正奖励函数，更具信息并指导agent到达目标，即$r_g(s,a)=- \mid s-g \mid ^2$。尽管使用修正奖励解决了我们玩具环境的问题，它应用到更复杂的环境中也许很困难。我们在4.4节实验研究了修正奖励的结果。  

![](/assets/Hindsight_Experience_Replay/Figure_1.png)  
图1. 位翻转实验  

　　替代修正奖励函数，我们提出一个不同的解法，不需要任何相关领域知识。考虑一个计算轮，状态序列为$s_1,...,s_T$以及目标序列$g \neq s_1,...,s_T$，意味着agent在每个时间步收到-1的奖励。我们的方法背后的关键想法是以一个不同的目标重新检查这个路径，既然这个路径无法帮助我们学习如何达到状态g，它一定告诉我们一些关于如何达到状态$s_T$的东西。这个信息通过使用离线策略强化学习算法并以st替换回放缓存中的g经历回放来加以利用。另外，我们仍然可以以原先的原始目标g进行回放。以这个修正，至少半数的回放路径包含不同于-1的奖励，学习变的更简单。图1比较了是否含有这个额外回放技术的DQN的最终性能，我们称为事后经验回放(HER)。没有HER的DQN只能解决n<=13的任务，有HER的DQN很容易解决n到达50的任务。实验细节设置见附录A。注记这个方法结合强力的函数近似器(如深度神经网络)允许agent学习如何达到目标g，甚至g在训练过程中从未出现过。  

　　我们在下节中更正式的描述我们的方法。  

### 3.2 多目标强化学习

　　我们的兴趣点在训练学习达到多个不同目标的agent。我们遵循Unversal Value Function Approximators(Schaul等人，2015a)的方法，即我们训练策略和值函数，将状态$s \in S$和目标$g \in G$同时作为输入。更进一步，我们展示训练agent执行多个任务比训练它执行仅一个任务更容易，因此我们的方法甚至只有单个我们想让agent执行的任务时也是可用的(类似的情况有Pinto和Gupta(2016)观察到)。  

　　我们假设每个目标$g \in G$对应于某个判定$f:s\rightarrow \lbrace 0,1 \rbrace $，agent的目标是达到任何满足$f(s)=1$的状态s。这种条件下，当我们想要准确指定需要的系统状态时，我们使用$S=G$以及$f_g(s)=[s=g]$。目标也可以指定为状态的某些特征，如假设$S=R^2$，且我们想要在给定x坐标值的情况下可以达到任何状态。这种情况下$G=R$，$f_g((x,y))=[x=g]$。  

　　更进一步，我们假设给定状态s，我们可以容易的找到一个满足这状态的目标g。更正式的，我们假设有一个给定的映射$m:S \rightarrow G$使得任意$f(s)=1$。注记这个假设并不苛刻且经常可以满足。这种情况下，每个目标对应一个我们想达到的状态，即$G=S$且$f(s)=[s=g]$，映射m只是一个单位映射。对于上段中2维状态和1维目标情况，这个映射也同样很简单$m((x,y))=x$。  

　　全局策略可以使用任意强化学习算法进行训练，通过从某个分布中抽样目标和初始状态，运行agent一些时间步，当目标没有达到时在每个时间步给出负奖励，即$r_g(s,a)=-[f_g(s)=0]$。然而这在实际中并不是很好，因为这个奖励函数稀疏且没多少信息。为了解决这个问题我们引入事后经验回放技术，这是我们的方法的关键。  

### 3.3 算法

　　事后经验回顾(HER)背后的想法非常简单，在经历某个计算轮$s_0,s_1,...,s_T$后，我们不仅将这个计算轮的原始目标和每个转移$s_t \rightarrow s_{t+1}$存入回放缓存，也将其他目标的子集存入。注意到追求的目标影响agent的动作而不是环境的动力学因素，因此我们可以以任意目标回放每个路径，假设我们使用离线策略强化学习算法如DQN,DDPG,NAF或者SDQN。  

　　为了使用HER，一个需要作出的选择是用于回放的额外目标集。我们的算法的最简单版本中，我们以$m(s_T)$回放每个路径，即在计算轮的最终状态达到的目标。我们在4.5节中用实验比较了不同类型和数量的额外回放目标。所有情况中我们也用原始的计算轮追寻的目标回放每个路径。更正式的算法描述见算法1。  

![](/assets/Hindsight_Experience_Replay/Algorithm_1.png)

　　HER可以视为一种隐式课程，用于回放的目标自然的从那些即使是随机agent也容易达到的目标平移到更复杂的目标。然而，相比于显示课程，HER不需要任何对初始环境状态分布的控制。HER不仅学习极端稀疏奖励，在我们的实验中，它在稀疏奖励上比修正奖励表现更好。这些结果是奖励修正的实际挑战的证明，奖励修正经常构成一个我们真正关心的度量的妥协(如二值的成功/失败)。  

## 4. 实验

　　展示我们的实验的视频链接：https://googl/SMrQnI。  

　　本节按如下顺序组织。在4.1节中，我们介绍试验中使用的多目标强化学习环境，以及训练流程。在4.2节中，我们比较是否含有HER时的DDPG的性能。在4.3节中，我们检查HER在单目标设置下是否改善了性能。在4.4节中，我们分析了使用修正奖励函数的效果。在4.5节中，我们比较了HER中抽样额外目标的不同策略。在4.6节中，我们展示了物理机器模型上的实验结果。  

### 4.1 环境

　　对于多目标强化学习来说，没有标准的环境，因此，我们建立了自己的环境。我们决定使用操作环境，基于一个已经存在的机器人硬件，保证我们面对的挑战尽可能与现实世界相对应。在所有实验中，我们使用7自由度机械臂，有一组平行的手指状夹具。机器用MuJoCo物理引擎进行模拟。整个训练过程在模拟环境中执行，但我们在4.6节显示训练好的策略在物理机器上表现良好，无需任何微调。  

![](/assets/Hindsight_Experience_Replay/Figure_2.png)
图2. 不同任务：推动(顶行)，滑动(中间行)和拿取-放置(底行)。红球表示目标位置。  

　　策略用含有ReLU激活函数的多层感知机(MLPs)表达。训练用Adam作为优化器的DDPG算法执行。为改进效率，我们使用8个子进程，每次更新后的平均参数。更多细节及所有超参数见附录A。  

　　我们考虑三个不同的任务：  
　　1.推动。这个任务中，一个盒子放在机器人面前的桌子上，任务是将它移动到桌子上的目标位置。机器手指被锁定防止抓取。学习到的行为是推动和翻转的混合。  
　　2.滑动。这个任务中，一个冰球放置在一个长的光滑桌面上，目标位置在机器到达的范围之外，因此它得打击冰球，使它滑动并由于摩擦停到适当的位置。  
　　3.拿取-放置。这个任务与推动类似，但目标位置在空中而且手指没有被锁住。为了在这个任务中的探索更容易，我们记录了一个盒子被抓取的状态，然后从这个状态开始后半段训练轮次。  

　　状态：系统的状态由MuJoCo物理引擎表达，包含所有机器组件的角度和速度，以及位置，转角和所有物体的速度。  
　　目标：目标描述了需要的目标(盒子或者冰球，取决于任务)位置以及固定的误差$\epsilon$，即$G=R^3$且$f_g(s)=[ \mid g-s_{object} \mid \leq \epsilon]$，此处$s_{object}$是状态s中目标的位置。HER中从状态到目标的映射简单的取$m(s)=s_{object}$。  
　　奖励：除非指定，我们使用二值且稀疏的奖励$r(s,a,g)=-[f_g(s')=0]$，此处s'是状态s中执行动作a后的状态。在4.4节中我们比较了稀疏和修正的奖励函数。  
　　状态-目标分布：对所有任务，夹具的初始位置是固定的，物体的位置和目标的位置是随机的。细节见附录A。  
　　观察：本段中，‘相对’意思是相对当前夹具位置。策略以夹具的绝对位置，物体和目标的相对位置，以及手指间的距离作为输入而给出。Q函数额外给出夹具和手指的直线速度，以及物体的相对直线速度和角速度。我们决定限制策略的输入，以便于更方便的部署到物理机器上。  
　　动作：我们考虑的问题中，没有需要夹具扭转的，因此我们让它固定。动作空间是4维的。3个维度确定在下个时间步需要的相对夹具位置。我们使用MuJoCo约束来移动夹具向需要的位置，但Jacobian-based控制也可以替代使用。最后一个维度确定两个手指之间需要的距离，以位置控制。  
　　回放中的用于抽样目标的策略：除非指定，HER使用目标相对于每个计算轮最终状态的回放，即$S(s_0,...,s_T)=m(s_T)$。我们在4.5节中比较选择那些目标进行回放的不同策略。  

### 4.2 HER改进性能了吗？

　　为了验证HER是否改进性能，我们在所有三个任务上评估了是否含有HER的DDPG。更进一步，我们比较了基于计数探索的DDPG。对于HER，我们在回放缓存中存储两次每个转移：一个一并存储了用于计算轮生成的目标，一个存储了相对计算轮最终状态的目标(我们成这个策略为final)。在4.5中，我们执行选择回放目标的不同策略S的简化模型研究，这里我们在图中使用4.5节的最好结果进行比较。  

![](/assets/Hindsight_Experience_Replay/Figure_3.png)
图3. 多目标设置下的学习曲线。一个计算轮为成功，对于推动和拿取-放置任务，如果物体和目标在计算轮结束时的距离小于7cm，对于滑动任务小于20cm。结果为5个随机种子的平均，阴影区域代表标准差。红线相对于4.5节的k=4的future策略，蓝线相对于final策略。  

　　从图3中，显然没有HER的DDPG不能解决任何任务，基于计数探索的DDPG只能在滑动任务上有小的进步。另一方面，有HER的DDPG几乎完美的解决了所有任务，它证明了HER是使从稀疏二值奖励中学习称为可能的关键元素。  

![](/assets/Hindsight_Experience_Replay/Figure_4.png)
图4. 单一目标情况学习曲线  

### 4.3 HER我们只关心一个任务时HER改进性能了吗？

　　本节中我们评估HER是否改进性能，在我们只关心一个目标的情况下。为了这个目的，我们重复上一节的实验，但目标状态在所有计算轮中是相同的。  
　　从图4中，很明显DDPG+HER表现比单纯DDPG要好，尽管目标状态在所有计算轮中是相同的。更重要的，比较图3和图4，我们也可以注意到，如果训练计算轮含有多个目标，HER学习的更快，因此实际中，它建议在多目标上训练，尽管我们只关心一个目标。  

### 4.4 HER与奖励修正如何相互作用？

　　至此我们只考虑形如$r(s,a,g)=-[ \mid g-s_{object} \mid > \epsilon]$的二值奖励。  
本节中，我们检查如果将这个奖励替换为一个修正奖励，是否有HER的DDPG性能如何变化。  
我们考虑奖励形式$r(s,a,g)=\lambda \mid g-s_{object} \mid ^p - \mid g-s'_{object} \mid ^p$，此处s'为在状态s中执行动作a之后的环境状态，$ \lambda \in \lbrace 0,1 \rbrace$, $p \in \lbrace 1,2 \rbrace$是超参数。  

　　图5显示结果。神奇的是DDPG和DDPG+HER都没能成功解决任何任务，在任何这种奖励函数下。成功的将强化学习应用到困难的不使用示范的操作任务中，通常需要更复杂的比我们尝试的更复杂的奖励函数，我们的结果与其一致。  

　　一下两个原因可以引起修正奖励性能如此之差：(1)我们优化的东西(如修正奖励函数)与成功条件(如在计算轮结束时物体在目标的某个半径内)之间有巨大的差异；(2)修正奖励惩罚了不适当的行为(如移动盒子到错误的方向)，这阻碍了探索。它使得agent学到，如果不能有价值的操作就根本不触碰箱子，我们在某些实验中注意到这种行为。  

　　我们的结果建议未知领域奖励修正并不有效(至少在我们尝试的简单形式中)。当然对于每个问题，存在一个奖励使其简单(Ng等人，1999)，但制作这个修正奖励需要很多领域知识，某些情况下不比直接描述策略更简单。这加强了我们的信念，就是从稀疏二值奖励中学习是很重要的问题。  

### 4.5 每个路径我们需要回放多少目标，如何选择他们？

　　本节中我们用实验评估了使用HER中选择目标的不同策略。至此为止我们用于回放的唯一额外的目标是相对于环境最终状态的目标，我们称这个策略为final。除此之外我们考虑如下策略：  
　　future——以来自同一个抽样计算轮，当转移被回放并被观察之后的k个随机状态，  
　　episode——来自同一计算轮，当转移被回放时的随机k个状态，  
　　random——在整个训练流程中至此累计的状态中k个随机状态。  

![](/assets/Hindsight_Experience_Replay/Figure_5.png)
图5. 修正奖励r(s,a,g)=-|g-s|^2的学习曲线(在我们尝试的修正奖励中它表现最好)。两个算法在所有任务上失效。  
![](/assets/Hindsight_Experience_Replay/Figure_6.png)
图6. 为回放选择额外目标的不同策略的简化模型实验。顶行显示训练轮次的最高测试性能，底行显示所有训练轮次的平均测试性能。右上图示为final，episode和future三者重合，由于所有这些策略在这个任务上达到完美表现。  

　　所有这些策略有超参数k，控制回放缓存中来自正常经验回放的数据到HER数据的比率。  

　　比较不同策略和不同k值的图示如图6。从图中我们可以看出除随机策略之外的所有策略几乎完美解决推动和拿取-放置任务，不管k值多少。所有情况中，k为4或8的future策略表现最好，它也是唯一能够完美解决滑动任务的策略。k=4的future策略学习曲线如图3。它证明回放的最有价值的目的是那些在近期将要达到的目的。注意到增加k值8时性能降低，由于缓存中正常回放的比率变得非常低。  

![](/assets/Hindsight_Experience_Replay/Figure_7.png)
图7. 部署在物理机器上的拿取-放置策略  

### 4.6 部署在物理机器上

　　我们得到在模拟器上训练的拿取-放置任务策略，将其部署在物理机器上，没有任何微调。盒子位置使用单独的训练好的CNN和原始的机器头部摄像图像进行预测。细节参见附录B。  

　　开始策略在5次中成功2次。它对盒子位置评估的小的误差不强壮，因为他在模拟器中的完美状态下进行训练。在用Gauss噪音加入观察后重新训练策略，成功率增加到5/5。展示一些测试的视频链接：https://goo.gl/SMrQnI。  

## 5. 相关工作

　　经验回放技术有Lin(1992)引入，在用于DQN玩Atari游戏后变得非常流行。优先级经验回放是一个对经验回放的改进，在回放缓存中优先排序转移来加速训练。它与我们的工作正价，两个方法很容易组合。  

　　同时学习多任务策略在策略搜索上已经被充分探索，如Schmidhuber和Huber(1990)；Caruana(1998)；Da Silva等(2012)；Kober等(2012)；Devin等(2016)；Pinto和Gupta(2016)。为多任务学习离线策略值函数有Foster和Dayan(2002)和Sutton等人(2011)进行综述。我们的工作大部分基于Schaul等人(2015a)的工作，他考虑训练一个单一神经网络近似多值函数。学习同时执行多任务在Hierarchical Reinforcement Learning的框架下研究了很长时间，如Bakker和Schmidhuber(2004)；Vezhnevts等人(2017)。  

　　我们的方法可以视为隐式课程学习(Elman，1993；Hengio等人，2009)。由于课程现在经常用于训练神经网络(如Zaremba和Sutskever(2014)；Graves等人(2016))，课程通常总是手工设置的。自动课程生成由Schmidhuber(2004)提出，它使用规划搜索对这个问题构造了一个渐进优化算法。另一个有趣的方法是PowerPlay(Schmidhuber，3013；Srivastaa等人，2013)，是一个通用的自动任务选择框架。Graves等人(2017)考虑一个设置，有一个固定离散任务集，为自动课程生成经验评估不同策略。另一个方法由Sukhbaatar等人(2017)和Held等人(2017)引入，使用在策略和任务设置器之前自己运作，来自动生成当前策略可达到的范围内的目标状态，我们的方法与这些技术正交，可以与他们相结合。  

## 6. 结论

　　我们引入一个全新的技术叫‘事后经验回顾’，使强化学习算法用于稀疏二值奖励称为可能。我们的技术可以与任意离线策略强化学习算法联合，我们用DQN和DDPG进行实验证明。  

　　我们展示HER可以训练用机械臂推动、滑动、拿取-放置物体到特定目标的策略，同时通常的强化学习算法不能解决这些问题。我们也展示拿取-放置任务的策略在物理机器上执行相当好，不需任何微调。至今为止，这是第一次只使用稀疏二值奖励学习到如此复杂的行为。  

## 致谢

　　我们感谢Ankur Handa，Jonathan Ho，John Schulman，Matthias Plappert，Tim Salimans和Vikash Kumar，他们提供了本文之前版本的反馈。我们也要感谢Rein Houthooft和整个Open AI团队，与他们卓有成效的讨论，以及Bowen Baker，他进行一些额外的实验。  

## 参考文献
