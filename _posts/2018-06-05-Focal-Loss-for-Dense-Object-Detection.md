---
layout: post
title:  "Focal Loss for Dense Object Detection"
categories: ObjectDetection
tags:  ObjectDetection FocalLoss
author: CQ
---

* content
{:toc}

## 摘要：

　　至今最高准确度的目标检测器基于由R-CNN普及的两阶段方法，即在少量候选目标位置后应用一个分类器。与之相对，用于可能目标位置的规律且密集抽样的单阶段检测器有更快且更简单的潜力，但至今为止，在准确度上落后于两阶段检测器。本文中，我们研究了为什么是这种情况。我们发现，在训练密集检测器时遇到的极端前景－背景类别失衡，是核心的原因。我们计划通过改变标准交叉熵损失，使其减少正确分类样本的损失。我们全新的“Focal Loss”聚焦于在少量牢固样本上进行训练，防止训练过程中检测器被大量廉价负样本所淹没。为了检测损失的有效性，我们训练了一个密集检测器样本，叫RetinaNet。结果显示，使用“Focal Loss”时，RetinaNet与以前的单阶段检测器速度相当，同时在准确度上超越所有至今为止最新水平的两阶段检测器。代码链接: [https://github.com/facebookresearch/Detectron](https://github.com/facebookresearch/Detectron)。  




## 1.简介

　　现今最新水平的目标检测器基于两阶段及候选驱动机制。例如通俗的R-CNN，第一阶段产生少量候选目标位置，第二阶段使用卷积网络将每个候选位置分类为前景类别之一或背景类别。通过一系列改进，这种两阶段框架在COCO挑战赛基准测试中始终达到最高准确度。  

　　除了两阶段检测器的成功，很自然要问的问题是：一个单独的单阶段检测器能否达到类似的准确度？单阶段检测器应用在目标位置、尺寸和长宽比的规律密集抽样。最近单阶段检测器的研究，如YOLO和SSD，展示出有前景的结果，并产生准确度在最新水平两阶段检测器的10%~40%以内的检测器。  

　　本文将此准确度范围进一步推进：我们展示了一个单阶段检测器，首次达到复杂得多的两阶段检测器的最新水平COCO　AP得分，如Feature Pyramid Network (FPN) 或Mask R-CNN、Faster R-CNN的各种变体。为了达到此结果，我们认为训练阶段的类别失衡是阻止单阶段检测器达到最新水平准确度的主要障碍，并提出一个新的损失函数消除这个障碍。  

　　在类R-CNN检测器中，类别失衡通过两阶段级联和启发式抽样解决。提议位置阶段(如Selective Search, EdgeBoxes, DeepMask, RPN)迅速缩减候选位置的数量到一个小的数字(如1000~2000)，过滤掉大多数背景抽样。在第二个分类阶段，启发式抽样，如固定的前景－背景比例(1:3)或者online hard example mining (OHME)，被执行用来保证前景和背景之间的可接受的平衡。  

　　与之相对，单阶段检测器必须处理从图片中规律抽样出的更大的候选目标位置集合。实践中，这通常达到总计枚举100,000个位置，密集覆盖空间位置、尺寸和长宽比。类似的启发抽样也可以使用，但并不高效，因为训练过程仍然被容易分类的背景抽样所主导。这种低效在目标检测中是一个经典问题，典型的通过技术手段解决，如bootstrapping或hard example mining。  

　　本文中，我们提出一个新的损失函数，作为以前处理类别失衡方法的更高效的替代。这个损失函数是是一个动态比例交叉熵损失，随着正确分类的信心增加，比例因子缩减到0，如图1。直觉上，这个比例因子在训练过程中可以自动降低容易分类抽样的权重贡献，迅速聚焦与困难分类抽样。实验显示，我们提出的Focal Loss使我们能训练高准确度的单阶段检测器，显著超越使用启发式抽样或hard example mining进行训练的替代选择，两者是之前训练单阶段检测器的最新水平技术。最后，我们注意到Focal Loss的精确形式不是决定性的，我们展示了其他可以达到类似结果的实例。  

　　为了证明Focal Loss的有效性，我们设计了一个简单的单阶段目标检测器，叫RetinaNet，由于其对输入图片的目标位置的密集抽样而得名。其使用一个高效的网络内金字塔特征(feature pyramid network)，并使用锚点框(anchor boxes)。利用各种各样来自[22, 6, 28, 20]的最新的概念。RetinaNet高效且准确，我们最好的模型，基于ResNet-101-FPN主干网络，在COCO测试集AP得分39.1，运行速度5fps，远超之前的提交的单模型最好结果，不管是单阶段还是两阶段检测器，如图2。  

![](/assets/Focal_Loss_for_Dense_Object_Detection/Figure_1.png)
图1. 我们提出一个称为Focal Loss的全新损失，增加因子$(1-p_t)^\gamma$到标准cross entropy准则中。设置$\gamma\gt0$降低正确分类样本($p_t\gt0.5$)相关的损失，将更多焦点放在难的分类错误的样本。如我们的实验证明，Focal Loss使得在大量易分类背景样本出现情况下训练高准确度密集检测器成为可能。
![](/assets/Focal_Loss_for_Dense_Object_Detection/Figure_2.png)
图2. 在COCO test-dev上的速度(ms) versus 准确度(AP)。有了Focal Loss，我们简单的单阶段RetinaNet检测器超越所有之前的单阶段和两阶段检测器，包括源于[20]的成绩最好报道的Faster R-CNN系统。我们展示以ResNet-50-FPN(蓝圈)和ResNet-101-FPN(桔方块)在五个尺寸图像(400-800像素)的各种RetinaNet。忽略低准确度系统($AP\lt25$)，Retinanet组成所有现有检测器的上界，一个印象深刻的变体(没有展示)获得40.8的AP得分。细节在第5节给出。

## 2.相关工作

**经典目标检测器：** 滑动窗范式，即在密集图片网格上应用分类器，有着悠久而丰富的历史。最早的成功之一是LeCun等人的经典工作，LeCun将卷积神经网络应用于手写数字识别。Viola和Jones使用集成目标检测器用于人脸检测，使得这个模型得以广泛采用。HOG的引入和通道特征积分导致行人检测的高效方法。DPMs帮助密集检测器扩展到更通用目标类别并在在很多年中得到PASCAL上的最好结果。尽管滑动窗口方法在经典计算机视觉中是检测范式的代表，随着深度学习的重现，两阶段检测器很快主导了目标检测领域，如下所述。  

**两阶段检测器：** 现代目标检测的主导范式基于两阶段方法。作为先驱的Selective Search工作，第一阶段生成少量的包含所有目标候选提议集，同时滤掉主要的非法位置，第二阶段将提议分类为前景类/背景类。R-CNN将第二阶段的分类器更新为卷积网络，在准确度上产生巨大的进步，引导了现代目标检测领域。R-CNN改进了很多年，既在速度上也在通过应用目标提议学习。Regioin Proposal Networks (RPN)将生成提议和第二阶段分类器整合为一个卷积网络，形成Faster R-CNN框架。这个框架的多种扩展都有提出[20, 31, 32, 16, 16]。  

**单阶段检测器：** OverFeat是第一个基于深度网络的现代单阶段检测器。最近，SSD和YOLO重燃了单阶段方法的兴趣。这些检测器为速度进行调整，但他们的精度落后于双阶段检测器。SSD的AP得分低10-20%，YOLO专注于更极端的速度/精度交换。如图2。最近研究表明，两阶段检测器可以简单的通过降低输入图像分辨率和提议数量而变得更快，但单阶段方法即使有更大的计算资源也在准确度上落后。与此相反，本文工作的目的是研究单阶段检测器可否在准确度上达到或超过两阶段检测器，同时运行在类似或更快的速度上。  

　　我们的RetinaNet和以前的密集检测器共享许多相似之处，尤其是RPN引入的锚点(anchors)概念以及如SSD和FPN中features pyramids的应用。强调一下，我们的简单检测器达到排名最高的结果不是基于网络的创新，而是源于全新的损失。  

**类别失衡：** 经典单阶段检测方法，如集成检测器和DPMs，和更现代的方法，如SSD，在训练过程中都面临巨大的类别失衡。这些检测器每张图片评估$10^4~10^5$个候选位置但只有几个位置含有目标。这种不平衡引起两个问题：(1)训练低效，由于大多数位置是廉价的负样本没有有用的学习信号贡献；(2)整体上，廉价的负样本会淹没训练导致退化的模型。一个通常的解决方式是执行某种形式的严格负样本挖掘(hard negative mining)，在训练过程中抽样牢固样本，或者更复杂的抽样/二次权重方案。与之相反，我们提出的Focal Loss自然的处理单阶段检测器面对的类别失衡，允许我们高效训练不经抽样的所有样本，同时没有廉价负样本淹没损失和计算梯度。  

**强壮性评估：** 已经有过很多关注在强壮的损失函数上，通过减少大误差的样本的损失减少类别外的点的贡献。与之相反，与其处理类别外的点，Focal Loss如此处理类别失衡：通过降低类别内的(廉价样本)点的损失使他们对总损失的贡献很小，尽管他们的数量很大。换句话讲，Focal Loss以与强壮损失函数原则相反的方式运行：专注于在少量牢固样本集上训练。  

## 3.Focal Loss

　　Focal Loss 用来处理在训练过程中有极端不平衡前景/背景类别(1:1000)的单阶段检测方案。我们从二值分类的cross entropy($CE$)引入Focal loss：  

$$
CE(p,y) = \begin{cases}
-log(p), \quad p=1 \\
-log(1-p), \quad otherwise
\end{cases}
$$  

上式中$y\in\lbrace-1,1\rbrace$指定真值类别，$p\in\[0, 1\]$是模型对标签y=1类别的估算概率。为记号方便，我们定义$p_t$：  

$$
p_t = \begin{cases}
p, \quad y=1 \\
1-p, \quad wtherwise
\end{cases}
$$  

然后将重写$CE(p,y)=CE(p_t)=-log(p_t)$。  

　　Cross Entropy Loss可以看做图1中的蓝线。从图中容易看出，这个损失的一个值得注意的性质是，即便很容易分类的抽样也会引入非平凡损失量。当对大量易分类样本求和时，这些小损失值会淹没稀少的真值类别。  

### 3.1 平衡交叉熵(Balanced Cross Entropy)

　　通常解决类别失衡的方法是引入一个权重因子$\alpha\in\[0, 1\]$，对类别1为$\alpha$，类别-1为$1-\alpha$。实际上，$\alpha$可以设置为类别频率的倒数，或作为超参数通过交叉验证集进行设置。为记号方便，我们以定义$p_t$类似的方式定义$\alpha_t$。$alpha$平衡交叉熵损失写为：  

$$
CE(p_t) = -\alpha_t\times log(p_t)
$$  

这个损失是$CE$的一个简单扩展，将其作为我们提出的Focal Loss的实验基准。  

### 3.2 Focal Loss定义

　　如实验将显示的，在密集检测器训练过程中巨大的类别失衡淹没了交叉熵损失。容易分类的负样本组成了损失的主要部分并主导了梯度。尽管$\alpha$平衡了正负样本的重要性，并不能区分容易/难分类样本。反之，我们改造了损失函数来降低易分类样本权重，因而聚焦在难分类负样本上训练。  

　　更形式化的，我们在cross entropy loss上增加一个调制因子$(1-p_t)^\gamma$，可调节的聚焦参数$\gamma \ge 0$。我们定义Focal Loss为：  

$$
FL(p_t) = -(1-p_t)^\gamma\times log(p_t)
$$  

　　对于不同值的$\gamma\in\[0, 5\]$，Focal Loss可视化如图1。我们注记Focal Loss的两个性质。(1)当一个样本被错误分类，$p_t$比较小，调制因子接近１，损失函数不受影响。随着$p_t\rightarrow1$，因子趋近于0，正确分类样本的损失被降低权重。(2)聚焦参数$\gamma$平滑的调整易分类样本权重降低的比率。当$\gamma=0$时，$FL$和$CE$等效，随着$\gamma$增加，调制因子的效果随之增加(我们发现在我们的试验中$\gamma=2$效果最好)。  

　　直觉上，调制因子降低了易分类样本的损失贡献并扩展了样本取得低损失的范围。例如，$\gamma=2$时，一个以$p_t=0.9$分类的样本相比$CE$有着低100倍的损失，以$p_t=0.968$分类的样本相比$CE$有着低1000倍的损失。这转而增加了修正错误分类样本(对于$p_t\lt0.5$且$\gamma=2$，其损失缩减接近４倍)的重要性。  

　　实际上，我们使用Focal Loss的$\alpha$平衡变体：  

$$
FL(p_t) = -\alpha_t\times (1-p_t)^\gamma\times log(p_t)
$$  

我们在实验中使用这个形式，它相对无$\alpha$平衡形式产生轻微的准确度改进。最后我们注记，损失层的实现将计算p的sigmoid操作和损失的计算联合，导致更大的数值稳定性。  

　　尽管在我们的主要实验中使用上述定义Focal Loss，它的精确形式并不严格。附录中我们考虑其他Focal Loss的示例，并证明这些同等有效。  

### 3.3 类别失衡和模型初始化

　　二值分类模型默认被初始化为有相等的输出$y=1$或者$y=-1$的概率。这种初始化下，出现类别失衡情况里，早期训练中频率高的类别的损失主导了整个损失并导致不稳定。为了克服这点，我们引入在训练开始阶段模型估算稀少类别(前景)$p$值的“先验”的概念。将先验记为$\pi$，将其设置为模型估算的稀少类别样本$p$值是低的，如0.01。我们注记这是在模型初始化里的一个改变，不是损失函数的。我们发现在严重类别失衡下，对cross entropy和Focal Loss的稳定性都有改善。  

### 3.4 类别失衡和两阶段检测器

　　两阶段检测器通常使用cross entropy loss进行训练，不使用$\alpha$平衡或我们提出的损失。相对的，它通过两种机制处理类别失衡：(1)两阶段级联以及(2)有偏的小批次抽样。第一个级联阶段是目标提议装置，将近乎无限的可能目标位置缩减到一两千。重要的是，选择提议并不是随机的，很可能与真实目标位置有关，这去掉了大量的容易负类别的主体。当训练第二阶段时，有偏的抽样用来构建小批次，例如包含1:3的正负样本比例。这个比例像一个通过抽样实现的隐藏$\alpha$平衡因子。我们提出的Focal Loss直接通过损失函数在单阶段检测系统中处理这些机制。  

## 4. RetinaNet检测器

　　RetinaNet是一个单独的统一的网络，由一个主干网络和两个任务专有子网络。主干用来在整个输入图像上计算卷积特征图，是一个通用卷积网络。第一个子网络在主干输出上执行卷积目标分类；第二个子网络执行卷积边界框回归。两个子网络以简单为特点，我们特定为单阶段密集检测器提出，如图3。尽管对这些部件有很多可能的选择，大多数参数对实验显示的精确值不是特别敏感。我们下面介绍RetinaNet的每个部分。  

**Feature Pyramid Network主干：**我们使用[20]中的Feature Pyramid Network (FPN)作为RetinaNet的主干。简单讲，FPN将标准卷积网络增加一个自上而下的通道和横向链接，网络有效的从单一分辨率图像中构建一个丰富的多尺度的特征图金字塔，如图3(a)-(b)。在不同尺度上，金字塔每一层都能用来检测目标。FPN改善了多尺度预测，从full convolutional network (FCN)，如其在RPN和DeepMask类型提议的收益所显示，同样也在两阶段检测器如Faster R-CNN或Mask R-CNN。  

　　参考[20]，我们在ResNet架构顶部构建FPN。我们构建一个有P3到P7层的金字塔，l表示金字塔层($P_l$层有比输入低$2^l$的分辨率)。如[20]中，所有金字塔层有C=256个通道。金字塔的细节大部分参考[20]，有少量适当的差别。由于许多选择并不严格，我们强调使用FPN主干是，使用仅从ResNet最后一层得到特征的初步实验产生低AP得分。  

**Anchors：**我们使用平移不变的anchor boxes，与RPN类似与[20]不同。archors在金字塔P3到P7层各自有$32^2$到$512^2$的面积。如[20]中，在每个pyramid层，我们使用三个比例{1:2, 1:1, 2:1}的archors。为了比[20]更密集的尺度覆盖，每一层我们增加了原始三个比例archors的${2^0, 2^\frac{1}{3}, 2^\frac{2}{3}}$大小的anchors。这在我们的设置中改善了AP得分。总计每层有A=9 anchors，考虑网络输入图像，交叉各层它们覆盖了32-813像素范围的尺度。  

　　每个anchor赋值为一个K长度one-hot分类目标向量，K是目标类别数，和一个4边界回归目标向量。我们使用RPN的分配原则，但为多类别检测进行修改并调整临界值。特别的，archors用0.5临界值的intersection-over-union　(IOU)赋值为真值目标框，如果IOU在[0, 0.4)以内则赋值为背景。由于每个anchor最多赋值给一个目标框，我们设置其K长度标签向量对应值为1其余值为0。如果一个anchor没有赋值，可能发生在覆盖范围[0.4, 0.5)区间内，它在训练过程中被忽视。边界框回归目标以每个archor与其赋值目标框之间的平移进行计算，如果没有赋值则被省略。  

![](/assets/Focal_Loss_for_Dense_Object_Detection/Figure_3.png)
图3. 单阶段RetinaNet网络，使用前向ResNet架构(a)顶部Feature Pyramid Network (FPN)作为主干产生丰富的多尺度卷积特征金字塔(b)。这个主干，RetinaNet增加两个子网络，一个为分类anchor boxes (c)，一个为将anchor boxes回归到真值目标框(d)。网络很简单，使其能聚焦于全新的Focal Loss函数，消除单阶段检测器和最新水平两阶段检测器如带有FPN的Faster R-CNN之间的准确度差距，同时运行速度更快。  

**分类子网络：**分类子网络在每一个空间位置对每一个A anchors和K目标类别预测目标出现概率。这个子网络是一个外挂于每个FPN层的小FCN；这个子网络的参数在所有pyramid层共享。它很简单。从一个给定pyramid层拿到一个C通道输入特征图，子网络应用四个3X3卷积层，每个卷积层有C个filters且每层跟有ReLU激活层，后跟一个有$K\times A$个filters的3X3卷积层。最后，sigmoid激活函数输出每个空间位置的$K\times A$二值预测，如图3(c)。大多数实验中我们使用C=256以及A=9。  

　　对比RPN，我们的目标检测子网络更深，仅使用3X3卷积，与边界框回归子网络(下述)不共享参数。我们发现这些更高层的决定比指定超参数值更加重要。  

**边界框回归子网络：**与目标分类子网络平行，为回归每个anchor box到临近的真值目标的平移，如果存在真值目标，我们添加另一个小FCN到每个金字塔层。边界框回归子网络与分类子网络相互独立，除了它在每个空间位置终止于4A线性输出，如图3(d)。对每个空间位置上A个anchors的每一个，这四个输出预测anchor和真值边框的相对平移。不像大多数近期研究，我们使用未知类别的边界框回归器，这使用更少的参数，而我们发现这同样有效。目标分类子网络和边界框回归网络使用独立的参数，尽管共享相同的结构。  

### 4.1 推测和训练

**推测：** RetinaNet由一个单一的FCN组成，包含一个ResNet-FPN主干、一个分类子网络和一个边界回归子网络，如图3。同样，推断涉及简单的将一张图片通过网络前向运算。为了提高速度，我们只对每个FPN层得分最高的1000个预测边界框进行解码，在临界值检测器可信度0.05之后。所有层的最高预测进行组合，应用临界值0.5的非最大值抑制，产生最后的预测。  

**Focal Loss：** 我们使用本文引入的Focal Loss作为分类子网络输出的损失。如将在第5节展示的，我们发现在实际中$\gamma=2$效果最好，RetinaNet在$\gamma\in\[0.5, 5\]$都很强壮。我们强调当训练RetinaNet时，Focal Loss应用于每个图片的所有约100,000个anchors。这与使用启发式抽样(RPN)或者hard example mining (OHEM, SSD)对每个小批次选择一个小anchor集合(如256)的通常的实现相反。一张图片的总Focal Loss是所有100,000个anchors的Focal Loss求和，被赋值为真值边界框的anchors的数量归一化。我们通过赋值anchors的数量进行归一化，不是所有anchors，因为anchors的巨大主体是易分类负样本，在Focal Loss下产生微不足道的损失值。最后我们注记，赋值给稀少分类的权重$\alpha$也有一个稳定区间，但它与$\gamma$相作用使得有必要两个一起选取(如表1a和1b)。通常随着$\gamma$增加$\alpha$会轻微降低($\gamma=2$, $\alpha=0.25$效果最好)。  

**初始化：** 我们用ResNet-50-FPN和ResNet-101-FPN进行实验。基准的ResNet-50和ResNet-101模型在ImageNet1k上进行预训练；我们使用[16]发布的模型。FPN新增加的层初始化如[20]。所有除RetinaNet子网络最后一层的新卷积层初始化为偏置$b=0$以及$\sigma=0.01$的高斯分布权重。对于分类子网络的最后一层，我们设置偏置初始化值为$b=-log(\frac{1-\pi}{\pi})$，$\pi$指定为在训练开始时，每个anchor都应以可信度$\pi$被标记为前景。所有试验中我们使用$\pi=0.01$，尽管对其精确值是强壮的。如3.3节解释，这种初始化阻止在训练的第一个迭代，大量背景anchors产生大的不稳定的损失值。  

**优化：** RetinaNet使用随机梯度下降(SGD)进行训练。我们在8GPU上使用并行SGD，每个小批次共16张图片(每个GPU两张)。除非特殊指定，所有模型以初始学习率0.01训练90,000迭代步，学习率在60,000和80,000迭代步时除以10。如无其他，我们使用水平图像翻转作为唯一的数据扩充形式。使用0.0001权重衰减和0.9动量。训练损失是Focal Loss和边界框回归使用的标准平滑L1损失的和。表1e中的模型训练时间范围在10~35小时。  

![](/assets/Focal_Loss_for_Dense_Object_Detection/Table_1.png)
表1. RetinaNet和Focal Loss (FL)的模型简化实验。所有模型在trainval135k上训练并在minival上测试，除非标注。如无特别指定，默认值为：$\gamma=2$；3尺寸3长宽比anchors；ResNet-50-FPN主干；600像素训练测试图像尺寸。(a)使用$alpha$-balanced CE架构的RetinaNet获得最多31.1的AP得分。(b)相反，在完全相同网络下使用FL给出2.9 AP得分提高，且对精确$\gamma/\alpha$设置相当强壮。(c)使用2-3尺寸和3长宽比的anchors产生好的结果，在此之后性能饱和。(d) Focal Loss超过最好的online hard example mining (OHEM)变体3分AP得分。(e)在test-dev上不同网络深度和图像尺寸RetinaNet的准确度/速度权衡(也见图2)。  

## 5. 实验

　　我们在COCO挑战赛基准测试的边界框检测赛道发布实验结果。对于训练，我们遵循公共练习，使用COCO trainval135k部分(train的80,000张图片和来自40,000张图片的val部分的随机35,000图片子集)。我们通过在minival部分(val部分的剩余5000张图片)上评估来报告系统缺陷和敏感性研究。对于主要的结果，我们在test-dev部分上报告COCO AP得分，它没有公共标签且需使用评估服务器。  

### 5.1 训练密集检测器

　　我们采用各种优化策略运行很多实验分析密集检测器损失函数的表现。对于所有实验，我们使用深度50或101的ResNet，顶部为Feature Pyramid Network (FPN)。对所有模型简化研究，我们在训练和测试中使用尺寸600像素的图片。  

**网络初始化：** 我们第一个训练RetinaNet的尝试，使用标准cross entropy loss并对初始化和学习策略没有任何修改。它很快失败了，网络在训练过程中发散。然而，简单的初始化模型的最后一层，使得检测目标的先验概率为$\pi=0.01$(见4.1节)，能得到有效的训练。使用ResNet-50和这个初始化训练的RetinaNet已经在COCO上产生可观的30.2 AP得分。结果对$\pi$并不敏感，所以我们对所有实验使用$\pi=0.01$。  

![](/assets/Focal_Loss_for_Dense_Object_Detection/Figure_4.png)
图4. 收敛模型中，不同$\gamma$值下正样本和负样本的归一化损失的累积分布函数。正样本中改变$\gamma$在损失分布上的效果很小。然而对于负样本，增加$\gamma$强烈地将损失集中在难分类样本上，几乎将所有注意远离易分类样本。

**平衡交叉熵(Balanced Cross Entropy)：** 我们的下一个改进学习的尝试涉及使用3.1节描述的$\alpha$-balanced CE loss。不同$\alpha$结果如表1a显示。选择$\alpha=0.9$得到0.9的AP得分增加。  

**Focal Loss：** 使用Focal Loss的结果如表1b显示。Focal Loss引入一个新的超参数，聚焦参数$\gamma$，控制调制项强度。当$\gamma=0$时，损失等效于$CE$损失。随着$\gamma$增加，损失形状发生改变，使得有低损失值的易分类样本更进一步折扣，如图1。随着$\gamma$增加，Focal Loss比$CE$有更大的收获。当$\gamma=2$，Focal Loss比$\alpha$-balanced CE loss产生2.9的AP得分改善。  

　　对于表1b中的实验，为了公平比较，我们对每个$\gamma$寻找了最好的$\alpha$。我们观察到更高的$\gamma$选择更低的$\alpha$(由于易分类负样本减少权重，更少的重要性需要放在正样本上)。总体上，改变$\gamma$的收益更大，确实最好的$\alpha$范围仅在$\[0.25, 0.75\]$(我们测试$\alpha\in\[0.01, 0.99\]$)。我们在所有实验中使用$\gamma=2.0$和$\alpha=0.25$，$\alpha=0.5$效果几乎同样好(低0.4 AP得分)。  

**Focal Loss的分析：** 为了更好的理解Focal Loss，我们分析了一个收敛模型损失的实际分布。为了这点，我们使用了默认的以$\gamma=2$(有36.0 AP得分)训练的ResNet-101 600像素模型。我们将这个模型用于大量的随机图片上，抽样预测概率约$10^7$负窗口和$10^5$正窗口。下一步，单独对于正负窗口，我们对这些样本计算Focal Loss，归一化损失使其和为1。有这些归一化损失，对于正样本和负样本或者对于不同设置的$\gamma$(尽管模型以$gamma=2$训练)，我们可以将损失进行从最低到最高的排序并画出其累计分布函数。  

　　正负样本的累计分布函数如图4所示。如果我们观察正样本，可以看到对于不同$\gamma$值其CPD看起来非常相似。例如，约前20%最难分辨样本计算出大约一半的正损失，随着$\gamma$增加，前20%样本的贡献更多，但影响较小。  

　　$\gamma$在负样本上的影响截然不同。对于$\gamma=0$，正负样本的CPD基本相似。然而随着$\gamma$增加，实质上更多的权重贡献于难分辨负样本。实际上，对于$\gamma=2$(默认设置)，损失的大部分主体来自小部分样本。如所见，Focal Loss能有效忽略易分辨负样本的影响，将所有注意放在难分辨负样本上。  

**Online Hard Example Mining(OHEM)：** [31]提出通过用高损失样本构建小批次的方式改进两件段检测器的训练。特别的，在OHEM中每个样本由其损失值打分，然后使用非最大值抑制，由最高损失样本构建小批次。最大损失抑制临界值和批次大小是可调参数。就像Focal Loss，OHEM将更多重点放在错误分类样本上，但不像Focal Loss，OHEM完全放弃易分类样本。我们也使用SSD实现了一个OHEM的变体：在对所有样本使用非最大值抑制后，小批次被构建为强制1:3的正负样本比例，用来保证每个小批次有足够正样本。  

　　我们测试了在我们的单阶段检测器设置中这两个OHEM变体，有很大类别失衡。对于选择批次尺寸和非最大值抑制临界值，原始OHEM策略和“OHEM 1:3”策略的结果如表1d。这些结果使用ResNet-101，在这个设置下，我们的使用Focal Loss的基准得到36.0的AP得分。相反，OHEM最好的设置(无1:3比例，批次尺寸128，0.5非最大值抑制)达到32.8 AP得分。这里有3.2 AP分数差距，显示了对于训练密集检测器Focal Loss比OHEM更有效。我们注记我们尝试了其他参数设置和OHEM变体，但没获得更好的结果。  

**Hinge Loss：** 最后，在早期实验中，我们尝试用$p_t$上的Hinge Loss进行训练，即对于一个特定$p_t$值以上的损失设为0。然而，这不稳定，我们也没能达成有意义的结果。探索替代损失函数的结果在附录。  

![](/assets/Focal_Loss_for_Dense_Object_Detection/Table_2.png)
表2. 目标检测。单一模型结果(边界框AP得分) vs COCO test-dev上最新水平。我们展示了RetinaNet-101-800模型的结果，使用尺度振动进行训练，比表1相同模型长1.5倍训练时间。我们的模型获得最好的结果，超过单阶段和双阶段模型。对于速度versus准确度的详细细节见表1e和图2。

### 5.2 模型架构

**Anchor密度：** 单阶段检测系统一个最重要的因素是它覆盖的可能图像框有多密集。使用区域池化操作，两阶段检测器可以分类任意位置、尺寸和长宽比的图像框。相反，由于单阶段检测器使用一个固定抽样网格，一个流行的方法，为了在这些方法中达到高边界框收敛，是在每个空间位置使用multiple anchors，覆盖各种尺寸和长宽比的边界框。  

我们扫描了FPN中每个空间位置和每个pyramid层使用的anchors的尺寸和长宽比数量。我们考虑了从每个位置单一正方形anchor到每个位置12个anchors跨越４个子倍率尺度($\frac{2^k}{4}$，对$k\le3$)和三个长宽比[0.5, 1, 2]。用ResNet-50的结果如表1c。一个出乎意料好的AP(30.3)得分是使用仅一个正方形anchor得到。然而，当每个位置使用3尺寸三长宽比时，AP得分被改善接近４分(到34.0)。我们在本工作的其他所有实验中使用这个设置。  

最后，我们注记增加超过6-9 anchors不显示更进一步收益。因此尽管两阶段系统能分类图像中的任意边框，关于密度性能的饱和意味着更高的两阶段系统潜在密度也许不会提供优势。  

**速度 vs 准确度：** 更大的主干网络得到更高的精度，但也更降低推断速度。同样也对于输入图像尺寸(有图像较短边定义)。我们在表1e中展示了这两者的影响。图2中，我们画了RetinaNet的速度/准确度权衡曲线，并在COCO test-dev上将其与使用公开数据的其他近期方法进行对比。图像显示RetinaNet，使用Focal Loss，组成所有已存在方法的上界，除了低准确度系统。使用ResNet-101和600尺度图像的RetinaNet(简单起见，我们记为RetinaNet-101-600)达到了最近公布的ResNet-101 Faster R-CNN的准确度，同时每张图片运行122ms相比于172ms(两者都在Nvidia M40 GPU上测量)。使用更大尺寸让RetinaNet超越所有两阶段方法的准确度，同时仍然更快。为了更快的运行时间，有仅一个操作点(500像素输入)，此处使用ResNet-50-FPN改进了ResNet-101-FPN。处理高帧率系统将需要特定的网络，如[27]，这超过了本文的范围。我们注记在发表之后，更快更准确的结果可以通过[12]的Faster R-CNN的变体得到。  

### 5.3 与最新水平比较

　　我们在COCO挑战赛数据集上评估RetinaNet，将test-dev的结果与近期最新水平方法进行比较，包括单阶段和两阶段模型。结果如表2，我们的RetinaNet-101-800使用尺度振动进行训练，比表1e模型长1.5倍训练时间(得到1.3 AP得分增加)。与已存在的单阶段检测器相比，我们的方法获得整整5.9分AP得分差距(39.1对33.2)，与最接近的竞争者DSSD相比，同时更快，如图2。与最近两阶段方法相比，RetinaNet获得2.3分的差距，相比表现最好的基于Inception-ResNet-v2-TDM的Faster R-CNN模型。加上ResNeXt-32x8d-101-FPN作为RetinaNet主干，进一步改进额外1.7 AP得分，在COCO上超过40 AP得分。  

## 6. 结论

　　本工作中，我们将类别失衡确认为阻止单阶段检测器超越表现最好的双阶段检测器的主要障碍。为了处理这点，我们提出了Focal Loss，增加一个调制项到cross entropy loss中，为了将学习聚焦于难分类样本。我们的方法简单高效。我们通过设计一个全卷积单阶段检测器证明其效率，并报告大量实验分析，显示它达到最新水平的准确度和速度。源码可在如下地址得到：[https://github.com/facebookresearch/Detectron](https://github.com/facebookresearch/Detectron)。  

## 附录A：Focal Loss

　　Focal Loss的精确形式并不严格，我们现在展示一个FOcal Loss的替代实例，具有类似的性质并产生同样价值的结果。下述也给出Focal Loss性质的更多内在性质。

　　我们考虑与主文中稍有不同的cross entropy(CE)和Focal Loss (FL)。特别的，我们如下定义一个量xt：

$$
x_t = yx
$$

此处$y\in\lbrace+1, -1\rbrace$指定了如前所述的真值类别。然后我们写$p_t=\sigma(x_t)$(这与公式2中$p_t$的定义兼容)。当$x_t\gt0$时一个样本被正确分类，此情况下$p_t\gt0.5$。

　　现在我们可以用xt定义一种Focal Loss的替代形式。我们如下定义$p_t^*$和$F_L^*$：

$$
\begin{eqnarray}
p_t^* = \sigma(\gamma x_t + \beta) \\
F_L^* = -log(p_t^*) / \gamma
\end{eqnarray}
$$

$FL^* $有两个参数，$\gamma$和$\beta$，控制损失曲线的陡缓和平移。我们在图5$CE$和$FL$旁边画出两个选定$\gamma$和$\beta$设置的$FL^* $。可以看出，就像FL，选定参数的$FL^* $减少了指定为正确分类样本的损失。

　　我们使用前述同样的设置训练ResNet-50-600但用选定参数的$FL^* $换出FL。这些模型达到与那些用FL训练的模型接近相同的AP得分，如表3。换句话讲，$FL^* $是FL的一个合理的替代，在实践中表现很好。

　　我们发现各种$\gamma$和$\beta$设置给出好的结果。图7中，我们展示了RetinaNet-50-600的结果，$FL^* $采用广泛的参数集。损失图以颜色区分，有效设置(模型收敛且AP得分大于33.5)显示为蓝色。为简单我们使用$\alpha=0.25$。可以看到，减少正确分类样本($x_t\gt0$)权重是有效的。

　　更普遍的，我们认为任何有与FL或$FL^*$类似性质的损失函数都同样有效。

## 附录B：梯度

　　供参考，$CE$、$FL$和$FL^* $对x的梯度为：

$$
\begin{eqnarray}
\frac{dCE}{dx}=y(p_t-1) \\
\frac{dFL}{dx}=y(1-p_t)^\gamma(\gamma p_tlog(p_t)+p_t-1) \\
\frac{dFL^*}{dx}=y(p_t^*-1)
\end{eqnarray}
$$

选定设置的图示如图6。对所有损失函数，梯度对于高可信度样本趋于-1或0。然而，不像CE，对于FL和FL*的有效设置，只要$x_t\gt0$梯度就变得很小。

![](/assets/Focal_Loss_for_Dense_Object_Detection/Figure_5.png)
图5. $x_t=yx$变换后Focal Loss变体与cross entropy的比较。原始FL和替代变体FL*都降低了正确分类样本相关的损失($x_t\gt0$)  

![](/assets/Focal_Loss_for_Dense_Object_Detection/Figure_6.png)
图6. 图5中损失函数对x的导数

![](/assets/Focal_Loss_for_Dense_Object_Detection/Table_3.png)
表3. 对于选定设置，FL和FL*的结果 versus CE的结果

![](/assets/Focal_Loss_for_Dense_Object_Detection/Figure_7.png)
图7. 不同$\gamma$和$\beta$设置FL*的有效性。图像以颜色区分，有效设置用蓝色显示

## 参考文献：
