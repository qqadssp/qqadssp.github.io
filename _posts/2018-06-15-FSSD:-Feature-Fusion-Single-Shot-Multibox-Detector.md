---
layout: post
title:  "FSSD: Feature Fusion Single Shot Multibox Detector"
categories: ObjectDetection
tags:  ObjectDetection SSD FSSD
author: CQ
---

* content
{:toc}

**Intro:** arXiv 2017.12  

**Link:** [https://arxiv.org/abs/1712.00960](https://arxiv.org/abs/1712.00960)  

**Code:** [https://github.com/lzx1413/CAFFE_SSD/tree/fssd](https://github.com/lzx1413/CAFFE_SSD/tree/fssd)  




## 摘要：

　　SSD(Single Shot Multibox Detector)是最好的同时具有高准确度和快速的目标检测算法之一。然而SSD的feature pyramid detection method使它很难融合不同尺度的特征。本文中，我们提出FSSD(Feature Fusion Single Shot Multibox Detector), 用一个全新且轻量的特征融合模块强化的SSD，这个模块可以在SSD上显著的改进性能，只有一点点速度损失。在特征融合模块中，来自不同层的不同尺度特征连接在一起，后跟一些降采样模块得到新的feature pyramid，新的feature pyramid送往multibox detectors预测最后的检测结果。在PASCAL VOC 2007　test上，使用单个Nvidia 1080Ti，输入尺寸300×300，我们的网络可以在65.8FPS(frame per second)的速度下得到82.7mAP(mean average precision)得分。另外，在COCO上的结果也以较大的差距好于传统SSD。我们的FSSD同时在准确度和速度上超过很多最新水平目标检测算法。。代码链接: [https://github.com/lzx1413/CAFFE_SSD/tree/fssd](https://github.com/lzx1413/CAFFE_SSD/tree/fssd)。  

## 1.简介

　　目标检测是计算机视觉的核心任务之一。近几年来，很多基于卷积网络的检测器被提出，用来改善目标检测任务的准确度和速度。但目标检测中的尺度变化对所有检测器来说仍然是决定性的挑战。如图1，有一些方法被提出用来解决多尺度目标检测问题。图1a将卷积网络用于不同尺度图像生成不同尺度的feature maps，这是相当低效的方式。图1b只选择一个尺度的feature map，但生成不同尺度的anchors来检测不同尺度的目标。Faster RCNN, RFCN等等算法采取这种方法。但固定的感受域尺寸对于检测太大或太小的目标是一个限制。如图1c的top-down结构最近很流行，且已经证明在FPN、DSSD和SharpMask中表现很好。但逐个层融合特征不够高效，因为有很多层需要联合在一起。
　　基于卷积网络的目标检测器的主要权衡是目标识别和定位之间的矛盾。更深的卷积网络，feature maps可以代表更多的平移不变的语义信息，这有益于目标识别，但不利于目标定位。为了解决这个问题，SSD采用feature pyramid来检测不同尺度的目标。对于用VGG16作为主干的网络，特征步长8的Conv4_3用来检测小目标，特征步长64的Conv8_2用来检测大目标。这个策略是合理的，因为小目标在浅层不会丢失太多位置信息，大目标也可以在深层被很好的定位和识别。问题是，浅层网络产生的小目标的特征缺失足够的语义信息，这将导致小目标检测上的不佳表现。另外，小目标也严重依赖背景信息。图6第一行显示了一些SSD的对小目标的检测缺失。本文中，为了处理前述提及的问题，我们提出Feature Fusion SSD(FSSD)，在传统SSD上增加一个轻量且高效的特征融合模块。我们首先定义特征融合模块的框架，摘录在目标检测上特征融合性能的有决定效果的因素。根据第3节和第4节中的理论分析和实验结果，特征融合模块的架构定义如下：来自不同层的不同尺度的Feature被投影然后串联在一起，后跟一个Batch Normalization层归一化feature values。然后我们增加一些降采样模块生成新的feature pyramid，送给multibox detectors产生最后的检测结果。使用上述架构，我们的FSSD相比传统SSD改进很大的性能，只有一点速度开销。我们在PASCAL VOC数据集和MS COCO数据集上评测FSSD。结果显示FSSD可以以很大幅度改进传统SSD，尤其对于小目标，不用任何附加说明。另外，基于VGG网络FSSD也超过很多最新水平检测器，包括ION和Faster RCNN。特征融合模块在目标检测任务中也比FPN效果好。我们的主要贡献总结如下：
(1) 我们定义特征融合框架并研究不同的因素来确定特征融合模块。
(2) 我们引入一个全新的轻量的联合来自不同层feature maps的方法。
(3) 在大量高质量实验后，我们证明FSSD对传统SSD有显著的改善，只有一点点速度降低。FSSD可以在PASCAL VOC数据集和MS COCO数据集上获得最新水平表现。

## 2.相关工作

**基于卷积网络的目标检测器：** 得益于深度卷积网络的力量，目标检测器如OverFeat和R-CNN开始显示出令人瞩目的准确度改进。OverFeat在image pyramid上滑动窗口中，将卷积网络用作为特征提取器。R-CNN使用selective search或Edge boxes产生的region proposals，通过预训练的卷积网络生成regin-based feature，采用SVM进行分类。SPPNet采用spatial pyramid pooling layer，允许分类模块重新利用卷积特征而不管输入图像分辨率。Fast R-CNN用分类和位置回归损失end to end的训练卷积网络。Faster R-CNN将selective search替换为regin proposal network(RPN)。RPN从来产生候选边界框(anchor boxes)同时滤除背景区域。然后另一个小网络用来分类和边界框回归，基于这些候选区。R-FCN用position sensitive ROI pooling(PSROI)替换Faster RCNN中的ROI poolling，来改进检测器准确度和速度两方面的质量。最近，Deformable Convolution Netword提出可变性卷积和可变形PSROI来进一步加强FRCN得到更高的准确度。
除了region based检测器，也有一些高效的单阶段检测器。YOLO(you only look once)将输入图像分科为若干格子，在图像每个部分执行定位和分类。得益于这个方法，YOLO可以以非常高的速度进行目标检测，但准确度不令人满意。YOLOv2是YOLO的加强版，通过去掉全连接层并采用类似RPN的anchor boxes改进YOLO。
SSD是另一个高效的单阶段检测器。如图2a所示，SSD通过两个3×3卷积层预测类别得分和默认边界框的位置平移。为了检测不同尺度的目标，SSD增加了一系列逐渐变小的卷积层生成pyramid feature maps，并根据层的感受域尺寸设置相应的anchor尺寸。然后NMS(non-maximun suppression)用来后处理最后检测结果。由于SSD直接从原始的卷积网络feature maps检测目标，它可以达到实时目标检测，比大多数最新水平的目标检测器进行的更快。
　　为了改进准确度，DSSD建议为SSD+ResNet-101增加卷积转置层，来引入额外的大尺度背景环境。然而，速度会由于模型复杂度变慢。RSSD使用池化和串联的交叠串联来充分使用feature pyramid中层与层间的关系，以小的速度损失增强准确度。DSOD研究如何从零开始训练目标检测器，并用DenseNet架构高效的改进参数。
在卷积网络中使用特征融合的算法：有很多方法，尝试使用多层特征改进计算机视觉任务的性能。HyperNet、Parsenet和ION在预测结果前串联来自多层的特征。FCN、U-Net、Stacked Hourglass networks也使用跳跃连接将底层和高层feature maps进行关联，充分利用综合信息。SharpeMask和FPN引入top-down结构将不同层特征联合在一起改进性能。  

## 3.方法

　　卷积网络有出色的能力提取pyramidal feature hierarchy，这有更多的从低层到高层的语义信息。传统SSD将这些不同层的特征视为相同层，然后直接从他们生成目标检测结果。这个策略是SSD缺乏捕获局部细节特征和全局语义特征的能力。然而，检测器应该融合背景环境信息和目标的细节特征，来确定小目标。所以对于卷积网络目标检测器改善准确度，综合少量结构的特征是重要的解决方案。

### 3.1 特征融合模块

　　如第2节所述，已经有很多算法尝试观察并充分利用pyramidal features。最常见的方法类似图1c。这种类型的特征融合用于FPN和DSSD中，已被证明对卷积检测器改进很多。但这需要多个特征合并过程。如图1c所示，右侧的新特征只能融合来自左侧对应层特征和较高层的特征。另外，隐藏特征和多特征元素间操作也耗费了大量时间。我们提出一个轻量且高效的特征融合模块来解决这个任务。我们的动机是以合适的方式一次融合不同层特征，然后从融合的特征中生成feature pyramid。当考虑特征融合模块时有几个因素需要考虑。我们将在下节研究他们。设$X_i, i\in C$是我们想要融合的源feature maps，特征融合模块可以如下描述：

$$
X_f=\phi_f\lbrace\Gamma_i(X_i)\rbrace i\in C \\
X_p^'=\phi_p(X_f) p\in P \\
loc,loss=\phi_{c,l}(\cup \lbrace X_p^' \rbrace)
$$

此处$\Gamma_i$意为在串联在一起前每一个源feature map的变换函数。$\phi_f$为特征融合函数。$\phi_p$是生成pyramid features的函数。$\phi_{c,l}$是从提供的feature maps预测目标检测的方法。我们将注意力集中在是否应该被融合的层($C$)的范围、如何融合选定的feature maps($\Gamma$和$\phi_f$)，以及如何生成pyramid features($\phi_p$)。
$C$：在传统的基于VGG16的SSD300中，其作者选择VGG16的conv4_3、fc_7，并新增conv6_2、conv7_2、conv8_2、conv9_2来产生特征进行目标检测。相应的特征尺寸为38×38，19×19，10×10，5×5，3×3和1×1。我们认为空间尺寸小于10×10的feature maps几乎没有信息进行合并，所以我们将层的范围设定为conv3_3，conv4_3，fc_7，和conv7_2(我们将conv6_2的步长设为1，所以conv7_2的feature map尺寸为10×10)。根据4.1.1节的分析，conv3_3没有带来收益，所以我们并不融合这一层。
$\phi_f$：有两种主要的方法将不同的feature maps合并在一起：串联和元素求和。元素求和需要feature maps应该有相同的尺寸，意味着我们不得不将feature maps转换到相同的通道。由于这个要求限制了feature maps融合的灵活性，我们倾向于使用串联。另外，根据4.1.2节的结果，串联比元素求和能得到更好的结果。所以我们使用串联来组合特征。
$\Gamma$：为了以一种简单高效的方式串联不同尺度的特征，我们采用如下策略。首先1×1卷积层作用在每个源层上来降低特征维度。然后，我们设定conv4_3的feature map尺寸作为基准feature map尺寸，也就意味着最小特征步长为8。conv3_3产生的feature maps被降采样到38×38，通过2×2步长2的最大值池化层。对于尺寸小于38×38的feature maps，我们使用双线性插值调整feature maps尺寸到与conv4_3相同。通过这中方式，所有特征在空间维度都有相同的尺寸。
$\phi_p$：采用自传统SSD，我们使用pyramid feature map生成目标检测结果。我们测试三种不同结构，比较结果选取最好的那个。根据4.1.4节的结果，我们选择了一个由一些简单的提取feature pyramid组件组成的结构。

### 3.2 训练

　　有两种主要的训练方法可以选择。首先，由于FSSD是基于SSD的，我们可以采用训练好的SSD模型作为我们的预训练模型。对于学习率，我们设置新的特征融合模块学习率比其他参数大两倍。另一种训练FSSD的方式是与训练传统SSD相同。根据表2中的实验(第2和5行)，这两种方式在最后结果上几乎没有区别，但从VGG16开始是训练比从SSD模型开始训练好一点点。但如图4显示，从SSD开始训练比从预训练VGG16模型开始训练首先的更快。另外，在相同的超参数下FSSD也比传统SSD收敛更快。

如实验将显示的，在密集检测器训练过程中巨大的类别失衡淹没了交叉熵损失。容易分类的负样本组成了损失的主要部分并主导了梯度。尽管$\alpha$平衡了正负样本的重要性，并不能区分容易/难分类样本。反之，我们改造了损失函数来降低易分类样本权重，因而聚焦在难分类负样本上训练。  

　　更形式化的，我们在cross entropy loss上增加一个调制因子$(1-p_t)^\gamma$，可调节的聚焦参数$\gamma \ge 0$。我们定义Focal Loss为：  

$$
FL(p_t) = -(1-p_t)^\gamma\times log(p_t)
$$  

　　对于不同值的$\gamma\in\[0, 5\]$，Focal Loss可视化如图1。我们注记Focal Loss的两个性质。(1)当一个样本被错误分类，$p_t$比较小，调制因子接近１，损失函数不受影响。随着$p_t\rightarrow1$，因子趋近于0，正确分类样本的损失被降低权重。(2)聚焦参数$\gamma$平滑的调整易分类样本权重降低的比率。当$\gamma=0$时，$FL$和$CE$等效，随着$\gamma$增加，调制因子的效果随之增加(我们发现在我们的试验中$\gamma=2$效果最好)。  

　　直觉上，调制因子降低了易分类样本的损失贡献并扩展了样本取得低损失的范围。例如，$\gamma=2$时，一个以$p_t=0.9$分类的样本相比$CE$有着低100倍的损失，以$p_t=0.968$分类的样本相比$CE$有着低1000倍的损失。这转而增加了修正错误分类样本(对于$p_t\lt0.5$且$\gamma=2$，其损失缩减接近４倍)的重要性。  

　　实际上，我们使用Focal Loss的$\alpha$平衡变体：  

$$
FL(p_t) = -\alpha_t\times (1-p_t)^\gamma\times log(p_t)
$$  

我们在实验中使用这个形式，它相对无$\alpha$平衡形式产生轻微的准确度改进。最后我们注记，损失层的实现将计算p的sigmoid操作和损失的计算联合，导致更大的数值稳定性。  

　　尽管在我们的主要实验中使用上述定义Focal Loss，它的精确形式并不严格。附录中我们考虑其他Focal Loss的示例，并证明这些同等有效。  

### 3.3 类别失衡和模型初始化

　　二值分类模型默认被初始化为有相等的输出$y=1$或者$y=-1$的概率。这种初始化下，出现类别失衡情况里，早期训练中频率高的类别的损失主导了整个损失并导致不稳定。为了克服这点，我们引入在训练开始阶段模型估算稀少类别(前景)$p$值的“先验”的概念。将先验记为$\pi$，将其设置为模型估算的稀少类别样本$p$值是低的，如0.01。我们注记这是在模型初始化里的一个改变，不是损失函数的。我们发现在严重类别失衡下，对cross entropy和Focal Loss的稳定性都有改善。  

### 3.4 类别失衡和两阶段检测器

　　两阶段检测器通常使用cross entropy loss进行训练，不使用$\alpha$平衡或我们提出的损失。相对的，它通过两种机制处理类别失衡：(1)两阶段级联以及(2)有偏的小批次抽样。第一个级联阶段是目标提议装置，将近乎无限的可能目标位置缩减到一两千。重要的是，选择提议并不是随机的，很可能与真实目标位置有关，这去掉了大量的容易负类别的主体。当训练第二阶段时，有偏的抽样用来构建小批次，例如包含1:3的正负样本比例。这个比例像一个通过抽样实现的隐藏$\alpha$平衡因子。我们提出的Focal Loss直接通过损失函数在单阶段检测系统中处理这些机制。  

## 4. RetinaNet检测器

　　RetinaNet是一个单独的统一的网络，由一个主干网络和两个任务专有子网络。主干用来在整个输入图像上计算卷积特征图，是一个通用卷积网络。第一个子网络在主干输出上执行卷积目标分类；第二个子网络执行卷积边界框回归。两个子网络以简单为特点，我们特定为单阶段密集检测器提出，如图3。尽管对这些部件有很多可能的选择，大多数参数对实验显示的精确值不是特别敏感。我们下面介绍RetinaNet的每个部分。  

**Feature Pyramid Network主干：**我们使用[20]中的Feature Pyramid Network (FPN)作为RetinaNet的主干。简单讲，FPN将标准卷积网络增加一个自上而下的通道和横向链接，网络有效的从单一分辨率图像中构建一个丰富的多尺度的特征图金字塔，如图3(a)-(b)。在不同尺度上，金字塔每一层都能用来检测目标。FPN改善了多尺度预测，从full convolutional network (FCN)，如其在RPN和DeepMask类型提议的收益所显示，同样也在两阶段检测器如Faster R-CNN或Mask R-CNN。  

　　参考[20]，我们在ResNet架构顶部构建FPN。我们构建一个有P3到P7层的金字塔，l表示金字塔层($P_l$层有比输入低$2^l$的分辨率)。如[20]中，所有金字塔层有C=256个通道。金字塔的细节大部分参考[20]，有少量适当的差别。由于许多选择并不严格，我们强调使用FPN主干是，使用仅从ResNet最后一层得到特征的初步实验产生低AP得分。  

**Anchors：**我们使用平移不变的anchor boxes，与RPN类似与[20]不同。archors在金字塔P3到P7层各自有$32^2$到$512^2$的面积。如[20]中，在每个pyramid层，我们使用三个比例{1:2, 1:1, 2:1}的archors。为了比[20]更密集的尺度覆盖，每一层我们增加了原始三个比例archors的${2^0, 2^\frac{1}{3}, 2^\frac{2}{3}}$大小的anchors。这在我们的设置中改善了AP得分。总计每层有A=9 anchors，考虑网络输入图像，交叉各层它们覆盖了32-813像素范围的尺度。  

　　每个anchor赋值为一个K长度one-hot分类目标向量，K是目标类别数，和一个4边界回归目标向量。我们使用RPN的分配原则，但为多类别检测进行修改并调整临界值。特别的，archors用0.5临界值的intersection-over-union　(IOU)赋值为真值目标框，如果IOU在[0, 0.4)以内则赋值为背景。由于每个anchor最多赋值给一个目标框，我们设置其K长度标签向量对应值为1其余值为0。如果一个anchor没有赋值，可能发生在覆盖范围[0.4, 0.5)区间内，它在训练过程中被忽视。边界框回归目标以每个archor与其赋值目标框之间的平移进行计算，如果没有赋值则被省略。  

![](/assets/Focal_Loss_for_Dense_Object_Detection/Figure_3.png)
图3. 单阶段RetinaNet网络，使用前向ResNet架构(a)顶部Feature Pyramid Network (FPN)作为主干产生丰富的多尺度卷积特征金字塔(b)。这个主干，RetinaNet增加两个子网络，一个为分类anchor boxes (c)，一个为将anchor boxes回归到真值目标框(d)。网络很简单，使其能聚焦于全新的Focal Loss函数，消除单阶段检测器和最新水平两阶段检测器如带有FPN的Faster R-CNN之间的准确度差距，同时运行速度更快。  

**分类子网络：**分类子网络在每一个空间位置对每一个A anchors和K目标类别预测目标出现概率。这个子网络是一个外挂于每个FPN层的小FCN；这个子网络的参数在所有pyramid层共享。它很简单。从一个给定pyramid层拿到一个C通道输入特征图，子网络应用四个3X3卷积层，每个卷积层有C个filters且每层跟有ReLU激活层，后跟一个有$K\times A$个filters的3X3卷积层。最后，sigmoid激活函数输出每个空间位置的$K\times A$二值预测，如图3(c)。大多数实验中我们使用C=256以及A=9。  

　　对比RPN，我们的目标检测子网络更深，仅使用3X3卷积，与边界框回归子网络(下述)不共享参数。我们发现这些更高层的决定比指定超参数值更加重要。  

**边界框回归子网络：**与目标分类子网络平行，为回归每个anchor box到临近的真值目标的平移，如果存在真值目标，我们添加另一个小FCN到每个金字塔层。边界框回归子网络与分类子网络相互独立，除了它在每个空间位置终止于4A线性输出，如图3(d)。对每个空间位置上A个anchors的每一个，这四个输出预测anchor和真值边框的相对平移。不像大多数近期研究，我们使用未知类别的边界框回归器，这使用更少的参数，而我们发现这同样有效。目标分类子网络和边界框回归网络使用独立的参数，尽管共享相同的结构。  

### 4.1 推测和训练

**推测：** RetinaNet由一个单一的FCN组成，包含一个ResNet-FPN主干、一个分类子网络和一个边界回归子网络，如图3。同样，推断涉及简单的将一张图片通过网络前向运算。为了提高速度，我们只对每个FPN层得分最高的1000个预测边界框进行解码，在临界值检测器可信度0.05之后。所有层的最高预测进行组合，应用临界值0.5的非最大值抑制，产生最后的预测。  

**Focal Loss：** 我们使用本文引入的Focal Loss作为分类子网络输出的损失。如将在第5节展示的，我们发现在实际中$\gamma=2$效果最好，RetinaNet在$\gamma\in\[0.5, 5\]$都很强壮。我们强调当训练RetinaNet时，Focal Loss应用于每个图片的所有约100,000个anchors。这与使用启发式抽样(RPN)或者hard example mining (OHEM, SSD)对每个小批次选择一个小anchor集合(如256)的通常的实现相反。一张图片的总Focal Loss是所有100,000个anchors的Focal Loss求和，被赋值为真值边界框的anchors的数量归一化。我们通过赋值anchors的数量进行归一化，不是所有anchors，因为anchors的巨大主体是易分类负样本，在Focal Loss下产生微不足道的损失值。最后我们注记，赋值给稀少分类的权重$\alpha$也有一个稳定区间，但它与$\gamma$相作用使得有必要两个一起选取(如表1a和1b)。通常随着$\gamma$增加$\alpha$会轻微降低($\gamma=2$, $\alpha=0.25$效果最好)。  

**初始化：** 我们用ResNet-50-FPN和ResNet-101-FPN进行实验。基准的ResNet-50和ResNet-101模型在ImageNet1k上进行预训练；我们使用[16]发布的模型。FPN新增加的层初始化如[20]。所有除RetinaNet子网络最后一层的新卷积层初始化为偏置$b=0$以及$\sigma=0.01$的高斯分布权重。对于分类子网络的最后一层，我们设置偏置初始化值为$b=-log(\frac{1-\pi}{\pi})$，$\pi$指定为在训练开始时，每个anchor都应以可信度$\pi$被标记为前景。所有试验中我们使用$\pi=0.01$，尽管对其精确值是强壮的。如3.3节解释，这种初始化阻止在训练的第一个迭代，大量背景anchors产生大的不稳定的损失值。  

**优化：** RetinaNet使用随机梯度下降(SGD)进行训练。我们在8GPU上使用并行SGD，每个小批次共16张图片(每个GPU两张)。除非特殊指定，所有模型以初始学习率0.01训练90,000迭代步，学习率在60,000和80,000迭代步时除以10。如无其他，我们使用水平图像翻转作为唯一的数据扩充形式。使用0.0001权重衰减和0.9动量。训练损失是Focal Loss和边界框回归使用的标准平滑L1损失的和。表1e中的模型训练时间范围在10~35小时。  

![](/assets/Focal_Loss_for_Dense_Object_Detection/Table_1.png)
表1. RetinaNet和Focal Loss (FL)的模型简化实验。所有模型在trainval135k上训练并在minival上测试，除非标注。如无特别指定，默认值为：$\gamma=2$；3尺寸3长宽比anchors；ResNet-50-FPN主干；600像素训练测试图像尺寸。(a)使用$alpha$-balanced CE架构的RetinaNet获得最多31.1的AP得分。(b)相反，在完全相同网络下使用FL给出2.9 AP得分提高，且对精确$\gamma/\alpha$设置相当强壮。(c)使用2-3尺寸和3长宽比的anchors产生好的结果，在此之后性能饱和。(d) Focal Loss超过最好的online hard example mining (OHEM)变体3分AP得分。(e)在test-dev上不同网络深度和图像尺寸RetinaNet的准确度/速度权衡(也见图2)。  

## 5. 实验

　　我们在COCO挑战赛基准测试的边界框检测赛道发布实验结果。对于训练，我们遵循公共练习，使用COCO trainval135k部分(train的80,000张图片和来自40,000张图片的val部分的随机35,000图片子集)。我们通过在minival部分(val部分的剩余5000张图片)上评估来报告系统缺陷和敏感性研究。对于主要的结果，我们在test-dev部分上报告COCO AP得分，它没有公共标签且需使用评估服务器。  

### 5.1 训练密集检测器

　　我们采用各种优化策略运行很多实验分析密集检测器损失函数的表现。对于所有实验，我们使用深度50或101的ResNet，顶部为Feature Pyramid Network (FPN)。对所有模型简化研究，我们在训练和测试中使用尺寸600像素的图片。  

**网络初始化：** 我们第一个训练RetinaNet的尝试，使用标准cross entropy loss并对初始化和学习策略没有任何修改。它很快失败了，网络在训练过程中发散。然而，简单的初始化模型的最后一层，使得检测目标的先验概率为$\pi=0.01$(见4.1节)，能得到有效的训练。使用ResNet-50和这个初始化训练的RetinaNet已经在COCO上产生可观的30.2 AP得分。结果对$\pi$并不敏感，所以我们对所有实验使用$\pi=0.01$。  

![](/assets/Focal_Loss_for_Dense_Object_Detection/Figure_4.png)
图4. 收敛模型中，不同$\gamma$值下正样本和负样本的归一化损失的累积分布函数。正样本中改变$\gamma$在损失分布上的效果很小。然而对于负样本，增加$\gamma$强烈地将损失集中在难分类样本上，几乎将所有注意远离易分类样本。

**平衡交叉熵(Balanced Cross Entropy)：** 我们的下一个改进学习的尝试涉及使用3.1节描述的$\alpha$-balanced CE loss。不同$\alpha$结果如表1a显示。选择$\alpha=0.9$得到0.9的AP得分增加。  

**Focal Loss：** 使用Focal Loss的结果如表1b显示。Focal Loss引入一个新的超参数，聚焦参数$\gamma$，控制调制项强度。当$\gamma=0$时，损失等效于$CE$损失。随着$\gamma$增加，损失形状发生改变，使得有低损失值的易分类样本更进一步折扣，如图1。随着$\gamma$增加，Focal Loss比$CE$有更大的收获。当$\gamma=2$，Focal Loss比$\alpha$-balanced CE loss产生2.9的AP得分改善。  

　　对于表1b中的实验，为了公平比较，我们对每个$\gamma$寻找了最好的$\alpha$。我们观察到更高的$\gamma$选择更低的$\alpha$(由于易分类负样本减少权重，更少的重要性需要放在正样本上)。总体上，改变$\gamma$的收益更大，确实最好的$\alpha$范围仅在$\[0.25, 0.75\]$(我们测试$\alpha\in\[0.01, 0.99\]$)。我们在所有实验中使用$\gamma=2.0$和$\alpha=0.25$，$\alpha=0.5$效果几乎同样好(低0.4 AP得分)。  

**Focal Loss的分析：** 为了更好的理解Focal Loss，我们分析了一个收敛模型损失的实际分布。为了这点，我们使用了默认的以$\gamma=2$(有36.0 AP得分)训练的ResNet-101 600像素模型。我们将这个模型用于大量的随机图片上，抽样预测概率约$10^7$负窗口和$10^5$正窗口。下一步，单独对于正负窗口，我们对这些样本计算Focal Loss，归一化损失使其和为1。有这些归一化损失，对于正样本和负样本或者对于不同设置的$\gamma$(尽管模型以$gamma=2$训练)，我们可以将损失进行从最低到最高的排序并画出其累计分布函数。  

　　正负样本的累计分布函数如图4所示。如果我们观察正样本，可以看到对于不同$\gamma$值其CPD看起来非常相似。例如，约前20%最难分辨样本计算出大约一半的正损失，随着$\gamma$增加，前20%样本的贡献更多，但影响较小。  

　　$\gamma$在负样本上的影响截然不同。对于$\gamma=0$，正负样本的CPD基本相似。然而随着$\gamma$增加，实质上更多的权重贡献于难分辨负样本。实际上，对于$\gamma=2$(默认设置)，损失的大部分主体来自小部分样本。如所见，Focal Loss能有效忽略易分辨负样本的影响，将所有注意放在难分辨负样本上。  

**Online Hard Example Mining(OHEM)：** [31]提出通过用高损失样本构建小批次的方式改进两件段检测器的训练。特别的，在OHEM中每个样本由其损失值打分，然后使用非最大值抑制，由最高损失样本构建小批次。最大损失抑制临界值和批次大小是可调参数。就像Focal Loss，OHEM将更多重点放在错误分类样本上，但不像Focal Loss，OHEM完全放弃易分类样本。我们也使用SSD实现了一个OHEM的变体：在对所有样本使用非最大值抑制后，小批次被构建为强制1:3的正负样本比例，用来保证每个小批次有足够正样本。  

　　我们测试了在我们的单阶段检测器设置中这两个OHEM变体，有很大类别失衡。对于选择批次尺寸和非最大值抑制临界值，原始OHEM策略和“OHEM 1:3”策略的结果如表1d。这些结果使用ResNet-101，在这个设置下，我们的使用Focal Loss的基准得到36.0的AP得分。相反，OHEM最好的设置(无1:3比例，批次尺寸128，0.5非最大值抑制)达到32.8 AP得分。这里有3.2 AP分数差距，显示了对于训练密集检测器Focal Loss比OHEM更有效。我们注记我们尝试了其他参数设置和OHEM变体，但没获得更好的结果。  

**Hinge Loss：** 最后，在早期实验中，我们尝试用$p_t$上的Hinge Loss进行训练，即对于一个特定$p_t$值以上的损失设为0。然而，这不稳定，我们也没能达成有意义的结果。探索替代损失函数的结果在附录。  

![](/assets/Focal_Loss_for_Dense_Object_Detection/Table_2.png)
表2. 目标检测。单一模型结果(边界框AP得分) vs COCO test-dev上最新水平。我们展示了RetinaNet-101-800模型的结果，使用尺度振动进行训练，比表1相同模型长1.5倍训练时间。我们的模型获得最好的结果，超过单阶段和双阶段模型。对于速度versus准确度的详细细节见表1e和图2。

### 5.2 模型架构

**Anchor密度：** 单阶段检测系统一个最重要的因素是它覆盖的可能图像框有多密集。使用区域池化操作，两阶段检测器可以分类任意位置、尺寸和长宽比的图像框。相反，由于单阶段检测器使用一个固定抽样网格，一个流行的方法，为了在这些方法中达到高边界框收敛，是在每个空间位置使用multiple anchors，覆盖各种尺寸和长宽比的边界框。  

我们扫描了FPN中每个空间位置和每个pyramid层使用的anchors的尺寸和长宽比数量。我们考虑了从每个位置单一正方形anchor到每个位置12个anchors跨越４个子倍率尺度($\frac{2^k}{4}$，对$k\le3$)和三个长宽比[0.5, 1, 2]。用ResNet-50的结果如表1c。一个出乎意料好的AP(30.3)得分是使用仅一个正方形anchor得到。然而，当每个位置使用3尺寸三长宽比时，AP得分被改善接近４分(到34.0)。我们在本工作的其他所有实验中使用这个设置。  

最后，我们注记增加超过6-9 anchors不显示更进一步收益。因此尽管两阶段系统能分类图像中的任意边框，关于密度性能的饱和意味着更高的两阶段系统潜在密度也许不会提供优势。  

**速度 vs 准确度：** 更大的主干网络得到更高的精度，但也更降低推断速度。同样也对于输入图像尺寸(有图像较短边定义)。我们在表1e中展示了这两者的影响。图2中，我们画了RetinaNet的速度/准确度权衡曲线，并在COCO test-dev上将其与使用公开数据的其他近期方法进行对比。图像显示RetinaNet，使用Focal Loss，组成所有已存在方法的上界，除了低准确度系统。使用ResNet-101和600尺度图像的RetinaNet(简单起见，我们记为RetinaNet-101-600)达到了最近公布的ResNet-101 Faster R-CNN的准确度，同时每张图片运行122ms相比于172ms(两者都在Nvidia M40 GPU上测量)。使用更大尺寸让RetinaNet超越所有两阶段方法的准确度，同时仍然更快。为了更快的运行时间，有仅一个操作点(500像素输入)，此处使用ResNet-50-FPN改进了ResNet-101-FPN。处理高帧率系统将需要特定的网络，如[27]，这超过了本文的范围。我们注记在发表之后，更快更准确的结果可以通过[12]的Faster R-CNN的变体得到。  

### 5.3 与最新水平比较

　　我们在COCO挑战赛数据集上评估RetinaNet，将test-dev的结果与近期最新水平方法进行比较，包括单阶段和两阶段模型。结果如表2，我们的RetinaNet-101-800使用尺度振动进行训练，比表1e模型长1.5倍训练时间(得到1.3 AP得分增加)。与已存在的单阶段检测器相比，我们的方法获得整整5.9分AP得分差距(39.1对33.2)，与最接近的竞争者DSSD相比，同时更快，如图2。与最近两阶段方法相比，RetinaNet获得2.3分的差距，相比表现最好的基于Inception-ResNet-v2-TDM的Faster R-CNN模型。加上ResNeXt-32x8d-101-FPN作为RetinaNet主干，进一步改进额外1.7 AP得分，在COCO上超过40 AP得分。  

## 6. 结论

　　本工作中，我们将类别失衡确认为阻止单阶段检测器超越表现最好的双阶段检测器的主要障碍。为了处理这点，我们提出了Focal Loss，增加一个调制项到cross entropy loss中，为了将学习聚焦于难分类样本。我们的方法简单高效。我们通过设计一个全卷积单阶段检测器证明其效率，并报告大量实验分析，显示它达到最新水平的准确度和速度。源码可在如下地址得到：[https://github.com/facebookresearch/Detectron](https://github.com/facebookresearch/Detectron)。  

## 附录A：Focal Loss

　　Focal Loss的精确形式并不严格，我们现在展示一个FOcal Loss的替代实例，具有类似的性质并产生同样价值的结果。下述也给出Focal Loss性质的更多内在性质。

　　我们考虑与主文中稍有不同的cross entropy(CE)和Focal Loss (FL)。特别的，我们如下定义一个量xt：

$$
x_t = yx
$$

此处$y\in\lbrace+1, -1\rbrace$指定了如前所述的真值类别。然后我们写$p_t=\sigma(x_t)$(这与公式2中$p_t$的定义兼容)。当$x_t\gt0$时一个样本被正确分类，此情况下$p_t\gt0.5$。

　　现在我们可以用xt定义一种Focal Loss的替代形式。我们如下定义$p_t^*$和$F_L^*$：

$$
\begin{eqnarray}
p_t^* = \sigma(\gamma x_t + \beta) \\
F_L^* = -log(p_t^*) / \gamma
\end{eqnarray}
$$

$FL^* $有两个参数，$\gamma$和$\beta$，控制损失曲线的陡缓和平移。我们在图5$CE$和$FL$旁边画出两个选定$\gamma$和$\beta$设置的$FL^* $。可以看出，就像FL，选定参数的$FL^* $减少了指定为正确分类样本的损失。

　　我们使用前述同样的设置训练ResNet-50-600但用选定参数的$FL^* $换出FL。这些模型达到与那些用FL训练的模型接近相同的AP得分，如表3。换句话讲，$FL^* $是FL的一个合理的替代，在实践中表现很好。

　　我们发现各种$\gamma$和$\beta$设置给出好的结果。图7中，我们展示了RetinaNet-50-600的结果，$FL^* $采用广泛的参数集。损失图以颜色区分，有效设置(模型收敛且AP得分大于33.5)显示为蓝色。为简单我们使用$\alpha=0.25$。可以看到，减少正确分类样本($x_t\gt0$)权重是有效的。

　　更普遍的，我们认为任何有与FL或$FL^*$类似性质的损失函数都同样有效。

## 附录B：梯度

　　供参考，$CE$、$FL$和$FL^* $对x的梯度为：

$$
\begin{eqnarray}
\frac{dCE}{dx}=y(p_t-1) \\
\frac{dFL}{dx}=y(1-p_t)^\gamma(\gamma p_tlog(p_t)+p_t-1) \\
\frac{dFL^*}{dx}=y(p_t^*-1)
\end{eqnarray}
$$

选定设置的图示如图6。对所有损失函数，梯度对于高可信度样本趋于-1或0。然而，不像CE，对于FL和FL*的有效设置，只要$x_t\gt0$梯度就变得很小。

![](/assets/Focal_Loss_for_Dense_Object_Detection/Figure_5.png)
图5. $x_t=yx$变换后Focal Loss变体与cross entropy的比较。原始FL和替代变体FL*都降低了正确分类样本相关的损失($x_t\gt0$)  

![](/assets/Focal_Loss_for_Dense_Object_Detection/Figure_6.png)
图6. 图5中损失函数对x的导数

![](/assets/Focal_Loss_for_Dense_Object_Detection/Table_3.png)
表3. 对于选定设置，FL和FL*的结果 versus CE的结果

![](/assets/Focal_Loss_for_Dense_Object_Detection/Figure_7.png)
图7. 不同$\gamma$和$\beta$设置FL*的有效性。图像以颜色区分，有效设置用蓝色显示

## 参考文献：
