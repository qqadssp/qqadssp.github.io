---
layout: post
title:  "Large-Scale Study of Curiosity-Driven Learning"
categories: ReinforcementLearning
tags:  ReinforcementLearning, Curiosity
author: CQ
---

* content
{:toc}

**Intro:** ICLR 2019  

**Link:** [https://openreview.net/forum?id=rJNwDjAqYX](https://openreview.net/forum?id=rJNeDjAqYX)  

**Code:** [https://github.com/openai/large-scale-curiosity](https://github.com/openai/large-scale-curiosity)  




## 摘要：

　　强化学习算法依赖于仔细构造的来自agent外部的环境的奖励。然而，为每个环境声明手工设计密集奖励是困难且不可扩展的，催生了发展agent内部奖励函数的需要。好奇心就是这样一个内部奖励函数，使用预测误差作为奖励信号。本文中：(a)我们执行了第一个纯好奇心驱动学习的大规模研究，即没有任何外部奖励，在Atari游戏套件的54个标准测试环境中。结果显示出奇的好性能，与很多游戏中内部好奇目标与手工构造外部奖励高度综合的结果相当。(b)我们调查了使用不同特征空间计算预测误差的效果，显示随机特征对于很多流行的强化学习游戏标准测试是足够的，但学到的特征似乎更具扩展性(如超级马里奥的全新游戏关卡)。(c)我们证明了随机设置中基于预测的奖励的限度。游戏视频和代码链接：[https://github.com/openai/large-scale-curiosity](https://github.com/openai/large-scale-curiosity).  

## 1.简介

　　强化学习已经呈现为训练agents执行复杂任务的流行方法。在强化学习中，agent的策略通过最大化与任务相关联的奖励函数进行训练。奖励是agent外部的且对于它们所在的环境是特定的。大多数强化学习获得的成功，都是当这个奖励是密集的且形态很好的时候获得的，如视频游戏中的环境机制得分。然而，构造一个形态很好的奖励函数是一个出了名的人工问题挑战。一个修正外部奖励的替代方法是用内部密集奖励补充它，就是奖励由agent自己创造。内部奖励的例子包括‘好奇心’以及‘访问记录’，前者使用预测误差作为奖励信号，后者阻碍agent访问相同的状态。想法是这些内部奖励将通过指导agent进行足够的探索环境来找到下一个外部奖励，来桥接稀疏外部奖励。  

　　但如果情景根本没有外部奖励呢？这种环境并不如它听起来那么陌生。发展心理学家谈论内部驱动作为人的早期发展阶段的初始动力：婴儿似乎使用无目的探索来学习未来生活中有用的技能。还有很多其他例子，从Minecraft到游览动物园，这不需要外部奖励。事实上，有证据表明，在给定环境中只使用内部奖励预训练一个agent让它在调整到一个全新环境中的全新任务时学习更快。但至今为止，没有系统的对于只使用内部奖励学习的研究。  

　　本文中，我们进行大规模的横跨不同模拟环境范围的纯内部奖励驱动agent的经验研究。特别的，我们选择Pathak(2017)等人提出的基于环境机制的好奇心内部奖励模型，因为它可扩展且可并行，使它对于大规模实验很理想。核心想法是将预测给定当前状态下agent动作的后果的误差作为内部奖励，即agent学习到的环境机制的预测误差。我们在54个环境里彻底的调查了基于环境机制的好奇心：视频游戏，物理模拟和虚拟3D导航任务，见图1。  

![](/assets/Large_Scale_Study_of_Curiosity_Driven_Learning/Figure_1.png)  
图1. 本文中调查的54环境的剪影。我们展示agent不使用外部奖励或计算轮结束信号，只使用好奇心也能进步。视频结果以及代码链接：[https://github.com/openai/large-scale-curiosity](https://github.com/openai/large-scale-curiosity)。  

　　为了更好的理解好奇心驱动学习，我们进一步研究了决定其性能的关键因素。特别的，在高维原始观察状态(如图片)中预测将来状态是一个挑战性的任务，以及如最近工作所示，在辅助特征空间学习环境机制会改进结果。然而，如何选择这样一个嵌入空间是紧要且开放研究的问题。为保证环境机制的在线训练的稳定性，我们建议嵌入空间应当：1)维度是紧凑的，2)对观察保留充足的信息，以及3)是观察的一个稳定的函数。  

　　尽管系统地切除，我们检查了不同的编码agent观察的方式，使得agent可以在纯靠自己好奇心驱动时表现很好。这里‘表现很好’的意思是在环境中有目的有技巧的活动。这在一些情况下可以定量评估，通过测量外部奖励或者环境特定的探索的测量，或者定性的观察agent交互的视频。我们展示了通过一个随机网络编码观察是一个简单但出奇有效的技术，对于很多流行的强化学习标准测试下的好奇心都是这样。这也许说明许多流行的强化学习视频游戏测试的视觉复杂程度并不如通常想象的那样。有趣的是，我们发现尽管在用于训练的环境中随机特征对于好的性能是足够的，学习到的特征似乎泛化的更好(如超级马里奥中的全新游戏关卡)。  

　　本文的主要贡献是：(a)在大量环境中大规模研究好奇心驱动探索，包括Atari游戏套件，超级马里奥兄弟，Unity中的虚拟3D导航，多玩家乒乓，以及Roboschool环境。(b)广泛调查了用于学习基于环境机制的好奇心的不同特征空间：随机特征，像素，反向环境机制，变分自编码器，并评估在未见过环境中的泛化性。(c)分析一些基于直接预测误差好奇心形式的局限。我们看到如果agent自己是环境中随机性的来源，它可以在没有任何实质进步的情况下将奖励自己。我们在3D导航任务中经验性的证明了这个局限性，环境中agent控制了环境的不同部分。  

## 2.基于环境机制的好奇心驱动学习

　　考虑一个agent，看到一个观察$x_t$，做出一个动作$a_t$然后转到下一个观察为$x_{t+1}$的状态。我们想要以关于这个转换有多少丰富性的奖励$r_t$激励这个agent。为了提供这个奖励，我们使用包含一下因素的探索奖励：(a)一个将观察嵌入到表征$\phi(x)$的网络，(b)一个前馈网络，在前一个观察和动作$p(\phi(x_{t+1}) \mid x_t, a_t)$的条件下预测下一个状态的表征。给定一个转换元祖$\lbrace x_t, x_{t+1}, a_t \rbrace$，探索奖励被定义为$r_t=-\log p(\phi(x_{t+1}) \mid x_t, a_t)$，也叫作‘惊喜’。  

　　以最大化这个奖励训练的agent会倾向于高预测误差的转换，预测误差在agent花更少时间呆的区域或有复杂环境机制的区域更高。这种基于环境机制好奇心已经在某些情况下显示得表现良好，尤其是在嵌入空间学习环境机制而不是原始观察空间。本文中，我们更进一步探索了基于环境机制的好奇心。我们使用相对与固定方差Gauss密度的平方误差作为‘惊喜’，即$ \mid \mid f(x_t, a_t)-\phi(x_{t+1}) \mid \mid ^2$，其中$f$是学到的环境机制模型。然而，任何其他密度都可以使用。  

### 2.1 前馈环境机制的特征空间

　　考虑上述好奇心形式的表征$\phi$。如果$\phi(x)=x$，前馈环境机制在观测空间进行预测。一个好的特征空间选择能是预测任务更容易处理，且滤掉观察空间的不相关方面。但对于环境机制驱动好奇心，什么是一个好的特征空间？我们提出好的特征空间必须有的性质：  
　　**紧致** 特征应该容易通过(更)低维建模，并滤掉观测空间的不相关部分。  
　　**充分** 特征应该包含所有重要信息。否则，agent将在探索环境的一些相关方面时不能得到奖励。  
　　**稳定** 不稳定奖励使强化学习agent难于学习。必须探索的奖励引入不稳定性，因为新的东西随时间而变的旧且无聊。在基于环境机制好奇心形式中，有两个不稳定来源：前馈环境机制模型由于训练而随时间演化，和特征随着它们的学习而改变。前者是方法内置的，后者应该尽可能最小化。  

　　本文中，我们系统研究了几个特征学习方法的功效，简要总结如下：  
　　**像素** 最简单情况是$\phi(x)=x$，我们在观察空间中拟合前馈环境机制模型。像素是充分的，因为没有信息被扔掉，而且是稳定的，因为没有特征学习组件。然而，从像素进行学习是棘手的，因为观测空间可能很高维且复杂。  
　　**随机特征** 下一个最简单情况是，我们选择的嵌入网络，一个卷积网络，并在随机初始化后固定它。由于网络固定，特征是稳定的。特征可以被做得在维度上紧致，但没有这样做的要求。然而随机特征在充分性上失效。  
　　**变分自编码器(VAE)** 变分自编码器被引入用来对于观察数据x和有先验分布p(z)的隐藏变量z，使用变分推断拟合隐藏变量生成模型$p(x,z)$。方法调用一个推断网络$q(z \mid x)$近似先验分布$p(z \mid x)$。这是一个前馈网络，使用观察作为输入并输出均值和方差向量，描述对角线协方差的高斯分布。我们可以使用到均值的映射作为我们的嵌入网络\phi。这些特征将是观察的一个低维近似充分总结，但他们可能仍含有一些无关细节如噪音，随时间进行，由于VAE的训练，特征将改变。  

![](/assets/Large_Scale_Study_of_Curiosity_Driven_Learning/Table_1.png)  
表1. 不同特征空间的类别总结  

　　**逆环境机制特征(IDF)** 给定一个转换$(s_t,s_{t+1},a_t)$，逆环境机制任务是给定前一个状态$s_t$和下一个状态$s_{t+1}$来预测动作$a_t$。特征使用常规神经网络首先嵌入$s_t$和$s_{t+1}$进行学习。想法是学习到的特征应该对应于环境的在agent直接控制下的某方面。这个特征学习方法容易实现且对某些特定噪音是不变的(见Pathak(2017)的讨论)。一个潜在的缺陷是学到的特征可能不充分，就是他们不能表征agent不能直接影响的环境重要方面。  

　　这些特征的总结见表1。注记学到的特征并不稳定，因为它们的分布随学习过程而变化。一个获得稳定性的方法也许是预训练的变分自编码器网络或逆环境机制特征网络。然而，除非已经进入游戏，否则不可能得到游戏界面的表征数据来训练特征。一个方式是用随机动作收集数据，然而它对于agent开始的环境是有偏的，将不能进一步泛化。由于所有特征都引入一些想要特征的权衡，这变成经验问题，就是它们每个特征在不同环境中有多有效。  

### 2.2 纯以好奇心驱动训练agent时的实践考虑

　　决定上述特征空间仅仅是实现可操作系统的第一块拼图。这里，我们详细阐述我们学习算法中的重要选择。我们的目的是降低不稳定性使得学习更稳定且在跨环境时保持一致。通过下述考虑，对于不同的特征学习方法和环境，我们能在最小变化超参情况下使探索行为可靠的工作。  

　　**PPO** 最后，我们发现PPO算法是需要微小超参调整的强壮的学习算法，所以我们的实验全部用它。  
　　**奖励正则化(Reward normalization)** 由于奖励函数不稳定，正则化奖励的尺度是有用的，这使值函数能很快学习。我们通过将奖励除以折扣奖励的和的标准差的运行时估计来做这件事。  
　　**优势函数正则化(Advantage normalization)** 当用PPO训练时，我们在一个batch中正则化优势函数，使其均值为0方差为1。  
　　**观察正则化(Observation normalization)** 我们运行一个随机agent在目标环境上10000步，计算观察的均值和标准差，用它们正则化训练时的观察。这很有用，保证特征在初始时没有很小的方差，也保证特征在跨环境时有较少的变异。  
　　**更多的actors** 方法的稳定性通过增加使用的并行actors(这影响batch-size)的数量而极大的增加。当训练agent时，我们使用128并行运行相同的环境来手机数据。  
　　**正则化特征(Normalizing the features)** 在联合内部和外部奖励时，我们发现这很有用，保证内部奖励的尺度在跨状态空间时保持一致。我们通过特征嵌入网络中使用batch-normalizaiton来实现它。  

### 2.3 ‘死亡不是终结’：以无限时间范围贴现好奇心

　　一个重要的点是计算轮结束信号的使用，有时称为'done'，通常飞逸走关于真正奖励函数的信息(通常假设我们获得了我们对agent隐藏的以便测量纯探索的外部奖励信号)。如果我们不移除'done'信号，许多Atari游戏变得太简单。例如，一个简单的策略，当agent活着的时候在每个时间步给+1的人工奖励，在死了的时候给0，在很多游戏中这足够获得高分，如Atari游戏‘Breakout’中agent寻求最大化计算轮长度并得到其分数。在负奖励情况中，agent将尝试尽快结束计算轮。  

![](/assets/Large_Scale_Study_of_Curiosity_Driven_Learning/Figure_2.png)  
图2. 8个Atari游戏和超级马里奥兄弟上特征学习方法的比较。这些评估曲线显示了纯以好奇心训练的agent的平均奖励(和标准差)，没有外部奖励或计算轮结束信号。可以看到纯好奇心驱动agent能够在这些环境中收集奖励，训练时不用任何外部奖励。所有Atari游戏的结果在附录图8中。我们发现像素上训练的好奇心模型在任何环境中都不可行，VAE特征运行与随机或逆环境机制模型相当或更差。更进一步，逆环境机制训练的特征在55%的游戏中表现比随机特征好。这个分析的一个有趣结果是随机特征建模好奇心是一个简单但出奇强壮的基线，并似乎在一般的Atari游戏环境中运行良好。  

　　鉴于这点，如果我们项研究纯探索agent的行为，我们就不应该偏置它。在无限时间范围设置中(即折扣回报在计算轮结束时并不截断，总会使用价值函数进行收集)，死亡只是agent的另一个转换，仅在它太‘无聊’时进行规避。因此，我们移除'done'将agent探索的收益与极少量的死亡信号分离。实践上，我们确实发现agent在游戏中避免死亡，因为死亡将它带到游戏开始——一个它已经看到很多次而且它可以很好的预测环境机制的区域。这个微妙之处被之前展示无外部奖励实验的工作忽略了。  

## 3. 实验

　　我们所有的实验中，策略和嵌入网络都从像素直接处理。对于我们的实现细节如超参数和架构，参见附录A。除非特别指定，所有曲线都是三个不同随机种子运行的平均值，阴影区域是均值的平方误差。我们已经公开了所有环境中运行纯好奇心agent的代码和视频。  

### 3.1 无外部奖励的好奇心驱动学习

　　我们以将一个纯好奇心驱动学习扩展到大量无外部奖励环境中作为开始。我们选择总共54个不同模拟环境，如图1，包括48个Atari游戏，超级马里奥兄弟，2个Roboschool情景(学习Ant和Juggling)，两人Pong，2个Unity迷宫(有或没有agent控制的电视)。这个大规模分析的目的是调查如下问题：(a)当你在不同的无外部奖励游戏中运行一个纯好奇心驱动agent时发生了什么？(b)你期望这些agent进行什么样的行为？(c)在这些行为上，基于环境机制好奇心的不同特征学习的变种的效果是什么？  

　　**Atari游戏** 为了回答这些问题，我们从著名的Atari游戏开始，用不同的特征学习方法进行一整套实验。一个测量纯好奇心agent表现有多好的方式是测量它能获得的外部奖励，即agent玩儿的有多好。我们在图2中展示了8个常见Atari游戏的平均外部奖励的评估曲线，以及附录中图8的48个Atari套件的曲线。一个重要的说明是，外部奖励只用来评估，不用来训练。然而，这只是一个对于纯探索的表达，因为游戏奖励可以是任意的，也可能与agent根据好奇心的探索没有任何关系。  

　　第一个从曲线中注意到的是：它们大多数都在上升。这显示纯好奇心驱动agent在训练时在没有见到任何外部奖励，通常能学到获得表面奖励。为了理解这为什么发生，考虑'Breakout'。游戏的主要控制动作是不停用平板的打击弹球，但这并不得分。只当球撞击到一块砖(砖随后消失)的时候游戏分数才增加。但越多的砖被球撞击，剩余砖的模式变得越复杂，使agent更好奇而进一步探索，因此，得分是副产品。更进一步，当agent死亡时，砖块被重置为初始状态，这agent在之前已经见过许多次，因此可预测性非常好，所以agent尽量通过保持存活避免死亡重置来增加好奇性。  

　　这个好奇心奖励通常足够的事实是不愿看到的，这也许说明许多流行的强化学习测试平台根本不需要外部奖励。似乎游戏人员有意设置教程指导agent只通过好奇心通关任务。这也许能解释为什么在许多人类设计的环境中，好奇心类的目标与外部奖励结合的相当的好。然而，并不总是这样，有时好奇心agent能做的比随机agent还要差。这发生在外部奖励在agent探索时很少收集到，或agent没能有效探索的时候(见图8中的'Atlantis'和'IceHockey'游戏)。我们鼓励读者参照网站上agent的游戏视频，更好的理解学习得的技能。  

　　**特征学习方法比较** 我们在图2中比较了四个特征学习方法：原始像素，随机特征，逆环境机制特征，变分自编码器(VAE)特征。在原始像素上训练环境机制在所有环境中表现很差，而将像素编码到特征表现较好。这很合理，因为在像素空间很难学习好的环境机制模型，预测误差被小的无关细节所主宰。  

　　神奇的是，随机特征在各个任务中表现相当好，有时比学习到的特征还好。这好性能的一个原因是随机特征是被冻结(稳定)的，在其上学习的环境机制模型相对容易，因为目标稳定。通常上，随机特征在视觉观察足够简单时运行良好，随机特征能保留关于原始信号的充分信息，例如Atari游戏。一个IDF特征稳定超越随机特征的情景是关于泛化，如在一关上训练超级马里奥兄弟而在另一关测试(细节见3.2)。  

　　VAE方法也表现良好但有时不稳定，因此我们决定使用随机特征和IDF进行进一步实验。附录中图8的细节结果比较了全部Atari套件游戏的IDF和随机特征。为了量化学习表现，我们比较了我们的好奇心aget和随机agent。我们发现在75%的游戏中IDF好奇心agent比随机agent收集更多的游戏奖励，随机特征好奇心agent在70%的游戏中表现更好。更进一步，IDF在55%的游戏中表现比随机特征好。总体上讲，随机特征和逆环境机制特征总体上运行良好。进一步细节见附录。  

　　**超级马里奥兄弟** 我们在超级马里奥兄弟中比较不同特征学习方法，如图2。超级马里奥兄弟已经在小规模试验的外部奖励自由学习方面被研究，我们敏锐的想看看单独好奇心能让agent走多远。我们用一个更高效版本的马里奥模拟器，允许更长时间训练，同时保持观测空间，动作和游戏运行机制相同。由于100倍长度的训练和使用PPO优化，我们的agent能通过几个游戏关卡，显著改善之前的马里奥兄弟上的探索结果。  

![](/assets/Large_Scale_Study_of_Curiosity_Driven_Learning/Figure_3.png)  
图3. (a)左：超级马里奥兄弟不同batch size的随机特征方法的比较。结果没有使用外部特征。(b)中间：网球(Roboschool)环境中的击球数量。(c)右：多玩家Pong环境中的平均计算轮长度。图中的不连续跳跃相对于agent达到环境的极限——在环境中特定步数后Atari Pong模拟器的背景颜色开始随机循环并变得对agent的动作无响应。  

　　我们能通过下面更稳定的优化来更进一步推进纯好奇心agent的性能吗？一种方式是增加batch-size。我们增加了运行环境并行进程，从128到1024。我们在图3(a)展示了使用128和1024并行环境进程进行训练的比较。如图所示，用1024并行环境进程的大batch-size训练表现好很多。事实上，agent能探索绝大多部分游戏内容：发现11个不同的游戏关卡，发现隐藏房间并打败boss。注记图中x轴是梯度步的数量，不是帧数，由于这种大规模实验并不主张抽样效率，而是训练agent的性能。这个结果说明纯好奇心驱动agent的性能将由于基本强化学习算法(本情况下的PPO)训练的变好而改进。视频在网站上。  

　　**Roboschool网球** 我们修改了RoboSchool框架下的Pong环境，只有一个平板有两个球。动作空间是二维连续的，我们将每个维度的动作空间离散到5个格子里，得到总共25个动作。策略和嵌入网络都在像素观察空间进行训练(注记：没有状态空间)。这个环境比游戏中的玩具环境更难控制，但agent学到了当球到它的区域时阻止并击球。我们观察球的弹跳数量作为一个与环境交互的表达，如图3(b)。视频参见网站。  

　　**Roboschool Ant Robot** 我们也使用Ant环境进行探索，包含了在一个跑道上的有8个控制部件的Ant。我们再次离散了动作空间并在原始像素(非状态空间)上训练策略和嵌入网络。然而，这种情况下，并不容易测量探索，因为外部距离奖励测量沿跑道的前进距离，但纯好奇新agent可以自由的向任何方向移动。我们发现一个类走路的行为在好奇心驱动训练中单纯的出现了。我们建议读者参考结果视频，显示agent有意义的与环境交互。  

　　**两玩家Pong中的多agent好奇心** 我们已经看到纯好奇心驱动agent没有奖励情况下学习玩儿很多游戏，但我们好奇有多少行为是由于对手是严密编码策略的电脑所引起的。如果我们让两者都是好奇心驱动的玩家，会发生什么？为了解决这个，我们采用了两玩家Pong游戏，游戏的双方由两个好奇心驱动agent控制。我们共享了两个agent的初始层，但有不同的动作头部，即总动作空间是1玩家和2玩家动作的叉积。  

　　注记外部奖励是无意义的，因为agent在玩儿两边，因此，我们展示了计算轮长度。结果见图3(c)。从计算轮长度我们可以看出，agent学习更长的游戏进程，学习玩儿Pong不用任何老师——两边纯靠好奇心。事实上，游戏进程太长以致它们弄坏了Atari模拟器，引起颜色根本改变，这也弄坏了策略，如图所示。  

### 3.2 在马里奥兄弟新关卡上的泛化

　　前一节中，我们展示了纯好奇心agent能学习有效的探索并学到有用的技能，如游戏中的玩游戏行为，Ant中的行走等等。目前为止，这些技能都是在agent训练的环境中展示的。然而，一个发展无奖励学习的优势是能够通过展示泛化到全新环境的能力，应用到丰富的‘无标签’无奖励函数环境中。  

　　为了测试这一点，我们首先仅在马里奥兄弟第1-1关使用好奇心预训练了agent。调查了基于随机特征和IDF特征的好奇心模型泛化到全新马里奥关卡能泛化多好。图4中，我们展示了两个在一个马里奥关卡训练在另一个测试关卡微调的例子，并与在测试关卡进行全新训练的学习进行对比。所有情况的训练信号都只有好奇心奖励。第一种情况中，从1-1关到1-2关，除关卡之外的环境比赛(两者都是‘白天’环境，即蓝天背景)全局统计有不同的敌人，不同几何形状，不同的难度。我们看到这种情景中，两种方法的迁移性都很强。然而，迁移性能在第二个从1-1关到1-3关的情景中较弱。这是因为下一关是相当难的，因为有从白天到夜晚的色板平移，如图4。  

　　我们进一步注明IDF学习到的特征在两种情况下都有迁移，随机特征在第一种情况下迁移，但在第二种从白天到黑夜的情景中不迁移。这个结果也许说明尽管随机特征在训练环境运行良好，学到的特征似乎在全新关卡扩展性更好。然而，这需要将来在大量环境下的更多分析。总体上讲，我们发现一些有意义的证据显示，好奇心学习的技能帮助agent在全新环境中有效的探索。  

![](/assets/Large_Scale_Study_of_Curiosity_Driven_Learning/Figure_4.png)  
图4. 马里奥泛化实验。左边我们展示了1-1关到1-2关迁移的结果，右边我们展示了1-1关到1-3关的结果。每个图的下侧是源环境和目标环境的地图。所有agent以无外部奖励进行训练。  

![](/assets/Large_Scale_Study_of_Curiosity_Driven_Learning/Figure_5.png)  
图5. Unity环境中的平均外部奖励，以外部终止信号+好奇心奖励。注记，以仅外部奖励训练的曲线是常数零。  

### 3.3 有稀疏外部奖励的好奇心

　　至今为止我们的试验中，已经显示agent无任何外部奖励时能学到有用的技能，纯好奇心驱动。然而，许多情景中，我们也许希望agent执行一些特殊的我们感兴趣的任务。这经常由定义外部奖励传达给agent。当奖励是稠密的(如每帧的游戏得分)，经典强化学习处理的很好，内部奖励经常不会有性能上的帮助。然而，构造稠密奖励是一个挑战性的人工问题(细节见简介)。本节中，我们评估好奇心能在agent只有稀疏或终止奖励的时候有多好的帮助执行任务。  

　　**终止奖励** 对于很多真实问题，只有终止奖励可以得到，例如导航，只在找到你所找的东西时得到奖励。这是一个典型的经典强化学习性能很差的情况。因此，我们考虑Unity ML-agent框架中9房间迷宫的3D导航以及稀疏终止奖励。动作空间是李赛的，包括：向前移动，向左15度，向右15度和无动作。agent从1号房间开始，离含有目标的9号房间很远。我们比较了以外部奖励训练的agent和外部奖励+内部奖励训练的agent。仅含外部奖励(经典强化学习)的agent在所有尝试中从没发现目标，意味着得到任何有意义的梯度是不可能的。而外部奖励+内部奖励每次全部收敛到获得奖励。图5中的结果显示平凡PPO，PPO+IDF好奇心，PPO+随机特征好奇心的结果。  

　　**稀疏奖励** 在初步实验中，我们选择5个稀疏奖励的Atari游戏，比较外部奖励(经典强化学习)和外部奖励+内部奖励(我们的)的性能。5个游戏中的4个，好奇心奖励改进性能(见附录表2，分越高越好)。我们想要强调，这不是本文的关注点，这些实验只是为了完整。我们只是将外部奖励(系数1.0)和内部奖励(系数0.01)直接综合没有任何调整。我们将如何优化综合外部奖励和内部奖励留为一个未来的方向。  

## 4. 相关工作

　　**固有动机** 一族对agent的固有动机奖励基于预测误差，或与agent策略一起得到训练的前馈环境机制模型的改进。结果是，agent被驱动达到前馈机制模型难于预测的环境区域，同时模型在这些区域改进其预测。这个对抗且非稳定机制能获得复杂行为。最近很少有工作在这个纯探索无外部奖励领域。相关的大多数是那些使用特征空间的前馈环境机制模型，Stadie(2015)使用自动编码特征，Pathak(2017)使用逆环境机制任务训练的特征。这些与2.1节详细介绍的VAE和IDF大略相关。  

　　计数状态访问的平滑版本能被用于内部奖励。当结合外部奖励如Atari游戏Montezuma's Revenge时，基于计数的方法已经显示出非常强的结果，无外部奖励时也显示出明显的游戏探索。还不清楚那种情况中基于计数方法更倾向于基于环境机制的方法，本文中我们关注基于环境机制奖励，因为我们发现它可以直接扩展和并行。在我们的初步试验中，我们在已经存在的基于计数实现扩展到大规模研究中，没有充分的成功。  

　　无外部奖励或拟合函数学习也在进化计算中被广泛研究，其中它指的是‘新奇搜索’。事件的新奇性被定义为事件与之前事件中最近邻事件的距离，使用某种事件统计来计算距离。文献中一个有趣的发现是，不独自优化拟合，可以发现更多有趣的解。  

　　其他探索方法是与最大奖励函数相综合进行的，如使用值函数估计的不稳定，或者使用策略的扰动进行探索。Schmidhuber(2010)和Oudeyer & Kaplan(2009)；Oudeyer(2018)提供了一个伟大的一些固有动机方法早期工作综述。探索的替代方法包括Sukhbaatar(2010)，他们使用两agent的对抗游戏进行探索。在Gregor(2017)中，他们优化一个叫做能力的量，是一个agent拥有的对状态进行控制的测量。与本文同时的工作中，差异性被用来作为一个无奖励函数学习技能的测量Eysenbach(2018)。  

　　**随机特征** 本文的其中一个发现是随机特征出奇的效果，有大量关于随机投影和更广义随机初始化网络的文献。很多文献关注于使用随机特征进行分类，典型发现是同时随机特征在简单问题上运行良好，一旦问题变得相当复杂，学习的特征表现更好。同时我们希望这个模式对于基于环境机制的探索也是真的，我们有一些初步证据显示，在马里奥兄弟中学习的特征似乎到全新关卡泛化的更好。  

## 5. 讨论

　　我们展示了gent纯以好奇心驱动训练能学到有用的行为：(a)agent不用任何奖励就能玩很多游戏。(b)马里奥不用任何外部奖励能过11关。(c)行走类似行为出现在Ant环境中。(d)网球类似行为在Roboschool环境中。(e)配合行为在两人Pong游戏中，两面都是好奇心驱动agent。单着并不总是真的，因为有一些Atari游戏探索环境与外部奖励无关。  

　　更一般的，我们的结果说明，很多人类的游戏，外部奖励经常与寻找新奇性的目的相关联。  

　　**基于预测误差好奇心的局限** 一个更严重的潜在局限是处理随机机制。如果环境转换是随机的，那么即便有了完美的机制模型，期望奖励将是转换的熵，agent将寻找最高熵的转换。即便环境不真是速记的，由廉价学习算法，贫乏的模型类或局部观察引起的不可预计性，将导致相同的问题。我们并没有在我们的游戏实验中观察到这个效应，所以我们用一个环境说明这点。  

![](/assets/Large_Scale_Study_of_Curiosity_Driven_Learning/Figure_6.png)  
图6. 我们增加了一个noisy TV到3.3节Unity环境中。我们比较了有无电视时的IDF和随机特征。  

　　我们回到3.3节的迷宫，经验性的验证一个常见思想实验，叫做noisy-TV问题。想法是当采取一个动作时，环境中像随机换台电视这样的局部熵来源应该被证明是agent不可抵抗的。我们按照字面意思采用这个思想实验，增加一个电视到迷宫中，并伴有换台动作。图6我们展示了增加noisy-TV如何影响IDF和随机特征的性能。如期望，电视的出现大幅降低了学习，但我们注意如果运行时间足够长，agent确实有时收敛到始终获得外部奖励。我们经验性的展示随机将是一个问题，所以未来工作更有效的方式处理这件事是很重要的。  

　　**未来工作** 我们展示了一个简单且可扩展的方法，能在大范围不同的无奖励函数或计算轮结束信号的环境中学习有意义行为。本文一个出奇的发现是随机特征表现相当好，但学习的特征似乎泛化性更好。同时我们相信一旦环境充分复杂，学习的特征将变得越来越重要，我们将其留为未来工作进行探索。  

　　然而，我们更宽泛的目的，是展示我们能利用很多无标签(例如无人工奖励函数)环境来改进感兴趣任务上的性能。优良这个目标，在一般奖励函数环境中展示性能只是第一步，未来工作研究将无标签环境迁移到标签环境将很有希望。  

## 参考文献
