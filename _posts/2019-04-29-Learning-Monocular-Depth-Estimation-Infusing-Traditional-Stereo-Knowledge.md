---
layout: post
title:  "Learning Monocular Depth Estimation Infusing Traditional Stereo Knowledge"
categories: StereoVision
tags:  StereoVision, DepthEstimation
author: CQ
---

* content
{:toc}

**Intro:** CVPR 2019  

**Link:** [https://arxiv.org/abs/1904.04144](https://arxiv.org/abs/1904.04144)  

**Code:** [https://github.com/fabiotosi92/monoResMatch-Tensorflow](https://github.com/fabiotosi92/monoResMatch-Tensorflow)  




## 摘要：

　　从单一图像进行深度估计是一个有趣的且具有挑战性的问题，有着无数应用。最近的工作证明，这个任务可以不需要来自真值标签的直接监督而学习到，利用图像在序列上的合成，或者立体匹配。本文关注第二种情况，文中我们利用立体匹配改进单目深度评估。为此我们提出monoResMatch，一个全新的深度架构，用来从单一输入图像推测深度，通过综合不同视角特征，与输入图像水平对齐，在两者之间执行立体匹配。与以往利用这个原理的工作相反的是，我们的网络首次从零开始端到端训练。更进一步，我们展示了如何通过传统立体算法获得代理真值声明，如半全局匹配，使得更精确的进行单目深度评估，通过自监督近似，遏制昂贵的深度标签需求。详细的实验结果证实了monoResMatch架构和代理监督的协同性，达到自监督单目深度评估的最新水平。代码链接：[https://github.com/fabiotosi92/monoResMatch-Tensorflow](https://github.com/fabiotosi92/monoResMatch-Tensorflow).  

## 1.简介

　　推测一个感知场景的精确深度信息对于很多应用是极为重要的，如自动驾驶，增强现实和机器人。尽管如雷达和ToF技术非常普及，从图像获得深度经常是更好的选择。相比其他传感器，那些基于标准相机的设备潜在的有几项优势：便宜，高分辨率，适用于几乎任何场景。这个领域中，立体视觉是比较好的选择，从统一区域不同视角的两张或多张图像推测差距，Semi-Global Matching是流行的有效的算法来完成这个任务。然而，从单一图像推测深度尤其有趣，因为它不需要立体装置，并且克服一些双目测距的内在限制(如遮挡)。另一刚面，这是个极具挑战性的任务，由于其天然是病态的。不管如何，深度学习在这个任务上能过获得出色的结果，尽管与立体解决方案的最新水平有很大差距。单目深度评估的自监督学习范式变得非常流行，克服对昂贵的真值声明的需求，通常通过昂贵的主动传感器和人工后处理得到。以这个方法，卷及神经网络可以训练来解决深度评估问题，以来自立体匹配或单目序列的图像合成任务的方式。为此，使用立体匹配作为监督相比单目序列更加有效。尽管前者的流程更受限制，因为训练时立体的设置是必要的，它即不需要推测两帧间的相对位姿也不需要分割场景中的移动物体。更进一步，立体设置并不需要摄像头移动来提供有意义的监督，不像单目设置。自监督的其它意义包括，对于各种不同任务，提取代理标签代替更昂贵的声明。  

![](/assets/Learning_Monocular_Depth_Estimation_Infusing_Traditional_Stereo_Knowledge/Figure_1.png)  
图1. 单目深度方案总览。KITTI输入图像(上方)。monoResMatch估计深度图(下方)。  

　　本文中，我们提出monoResMatch，一个全新的端到端架构，训练从单目图像评估深度，利用虚拟立体配置。第一阶段，我们将输入图像映射到特征空间，然后我们用这个表征评估第一深度输出，随后与虚拟右图像一并合成特征。最后，最后微调模块在真是图像和合成特征之间执行立体匹配。其他运用类似原理的框架结合合成网络进行合成和立体，我们使用单一架构端到端训练，获得相比现有方法可观的准确度改进。更进一步，我们利用传统立体视觉知识得到准确的代理标签，通过立体匹配监督来改进单目深度评估。我们将展示，尽管生成标签出现异常点，对于自监督，根据这个流程训练得到了优秀的准确度，相比图像变形方法。KITTI原始数据集的实践结果显示，融合两个之前提到的流程的关键组件，能得到最新水平的结果，相比其他不需要任何真值声明的单目深度评估的自监督框架。图1显示了我们框架的总览，描述了输入帧和monoResMathc输出。  

## 2.相关工作

　　本节中，我们回顾我们的工作的相关文献，关于立体/单目深度评估和代理标签提取。  

**立体深度估计**

　　大多数传统稠密立体算法依赖于[46]描述的著名的四步中的一些或所有。这个领域中，SGM表现出色，出色的准确度和效率间的权衡，因此非常流行。Zbontar和Cun是第一个应用深度学习到立体视觉，利用一个叉状CNN网络替代传统匹配损失计算，网络训练来预测图像块之间的相似度。Luo将相关问题视为一个多类别分类任务，得到更好的结果。Mayer抛弃了之前的方法，提出端到端可训练网络名为DispNetC，直接从图像推断差距。由于DispNetC使用一个1-D关联模仿损失体积，Kendall的GCNet利用4-D体积上的3-D卷积得到匹配损失，最终应用一个微分版本的argmin来选择这个体积上的最佳差距。其他工作遵从这两个主要流程，从DispNetC或者GCNet开始构建更复杂的架构。影响这些架构的域偏移问题(如合成真实)以在线或离线的方式处理，或通过外部深度测量大幅降低。  

**单目深度估计**

　　在深度学习之前，一些工作通过MRF或集成分类的方法解决单目深度问题。然而，随着真值深度数据的增加，基于CNN的监督方法快速超过之前的技术。一个有趣的趋势是以自监督的方式进行单目深度学习的可能性，避免昂贵的真值深度标签的需求，以多视角感知数据进行替代。然后，监督信号可以通过根据评估深度或相机位姿或二者都有的图像合成得到。通常，从立体摄像头获得图像相比使用一个单一移动的摄像头能够更高效的训练，帧之间的位姿已知。考虑立体监督，Garg首先采取这个方法，Godard引入空间变换网络以及左右一致损失。其他方法改进效率，采用金字塔架构，通过模拟三目设置或引入关节语义分割来增加准确度。[38]中，提出一个流程，利用固定点量化降低[40]的能量效率。Kuznietsov的半监督框架联合LiDAR测量的立体监督。Zhou首次尝试用单摄像头序列监督单目深度框架。这个方法通过引入额外线索，如相关点云、可微分DOV、多任务学习，得到改进。Zhan联合列出的这两种用于立体视觉序列的监督方法。另一类方法应用生成对抗流程到单目情景中。  

　　最后，与我们工作相关的是单视角立体匹配(SVS)，使用Deep3D处理单一图像得到第二个组合视觉，然后使用DispNetC计算视差图。然而，这两个架构是独立训练的。更进一步，DispNetC由来自人工合成和真值域的真值标签监督训练。不同的是，我们要引入的框架不需要真值，且端到端训练，大幅超越SVS。  

![](/assets/Learning_Monocular_Depth_Estimation_Infusing_Traditional_Stereo_Knowledge/Figure_2.png)  
图2. monoResMatch架构图示。给定一个输入图像，第一阶段中多尺度特征提取器(红色)生成高阶表征。初始视差评估器(蓝色)生成立体匹配的左右帧多尺度视差图。视差微调模块(橘色)负责依据第一阶段特征和第二阶段视差调整初始视差，匹配输入图像的高维特征$F_L^0$和虚拟右视角合成特征$F_R^0$之间的损失，以及$F_L^0$和反向卷曲$F_R^0$之间的绝对误差$e_L$。  

**代理标签提取**

　　由于对于大多数任务，真值标签昂贵且难以获得，一些工作查看了用容易获得的代理标签替代的可能性。Tonioni提出适应深度立体网络到没见过的环境中，利用传统立体算法和置信测量，Tosi学习置信估计选择正向和负向匹配，通过传统置信测量，Makansi和Liu生成置信标签以训练光纤流网络，使用传统方法。尤其相关于单目深度估计的工作由Yang提出，使用立体视觉里程计训练单目深度评估，Klodt和Vedaldi，利用运动算法，以及Guo，通过从立体匹配推测视差图的监督训练深度网络获得标签。  

## 3.Monocular Residual Matching

　　本节中，我们详细描述了MonoResMatch的架构，用来推测准确且稠密的深度估计，以单张图片的自监督方法。图2概括了网络的三个关键组件。首先，一个多尺度特征提取器以单一原始图像为输入，并在不同尺度计算深度学习特征，从1/4分辨率$F_L^2$到全分辨率$F_L^0$，以锤炼网络的光学表现的双义性。第二，输入图像分辨率的深度高维特征被处理用于评估，通过一个含有跳跃链接，图像的多尺度逆向深度(视差)图，以及一个训练中学到的虚拟右视角。通过这样做，网络学到仿真双目设置，因此允许下一步立体领域的处理。第三，视差微调阶段评估对初始视差的残差修正。特别的，我们使用第一阶段的深度特征和虚拟右图像的反向卷曲特征来计算损失体积，存储使用修正层得到立体匹配损失。  

　　我们的整个架构从零开始端到端训练，SVS的两个主要组件，Deep3D和DispNetC，在图像合成和视差评估任务上单独训练(后者需要额外的来自合成图像的监督深度标签)。  

　　大量实验结果证明monoResMatch相比SVS和其他最新水平方法能够进行更准确的评估。  

### 3.1 多尺度特征提取器

　　由[25]启发，给定一个图像，我们使用卷积核生成深度表征。特别的，第一个2步长层使用64个$7 \times 7$的卷积核，第二哥2不成卷积层使用128个$4 \times 4$卷积核。两个卷积转置块，步长为2和4，用来从低空间分辨率到全分辨率向上抽样特征，每个产生32个特征图。步长1的$1 \times 1$卷层进一步处理向上抽样表征。  

### 3.2 初始视差评估

　　给定第一个模块提取的特征，这个组件负责评估初始视差图。特别的，一个由DispNet启发的编码-解码架构处理多尺度特征提取器的1/4分辨率的深度特征，输出不同分辨率的视差图，从1/128到全分辨率。每个降采样模块，有步长分别为2和1的两个卷积块，处理越来越增长的提取特征，分别为64,128,256,512,1024，每个卷积层都使用$3 \times 3$卷积核后跟ReLU非线性层。不同于DispNet，在这个阶段的早期使用立体匹配的左右图像特征计算匹配损失，我们的架构缺少这种用于计算提及损失的必要信息，因为它处理单一输入图像。因此，在我们工作的这个阶段非1维关联层用来编码几何约束。然后向上抽样模块通过跳跃链接用来丰富特征表达，并提取两个视差图，对于输入帧和其右侧虚拟视角，如[11]。这个过程在每个尺度上执行，使用步长1的$3 \times 3$卷积核。  

### 3.3 视差修正

　　给定一个从网络第二部分得到的每个尺度视差的初始评估，通常伴随这深度不连续和屏蔽区域这样的错误，这个阶段通过几个非线性层预测相关的多尺度残差信号，并计算最后的左视角相关视差图。这个流程允许我们简化整个网络的端到端学习过程。更进一步，由[30]启发，我们相信几何约束在集成最终深度准确度中扮演核心角色。由于这个原因，在特征空间中嵌入匹配损失，利用水平关联层，在深度立体算法中的典型使用方式。为此，我们依靠前述计算的右视角视差图，使用可微分双线性抽样来从做视角特征$F_L^0$生成右视角特征$F_R^0$。网络也以误差$e_L$为输入，即图像分辨率下的左特征和虚拟右特征的绝对差距，以及前者在相同坐标下的反向卷曲，如[24]。  

　　我们再次指出，不同于[30]，我们的架构即产生合成右视角，及它的特征表达，也以立体合理性计算最终视差图。这使得monoResMatch是一个单一端到端架构，有效的从单一输入视角执行立体视觉，而不是[31]中联合两个单独训练的模型。更进一步，大量实验标示出我们全自监督端到端方法的卓越的准确度。  

### 3.4 训练损失

　　为了训练我们的多阶段架构，我们定义总损失为两个主要贡献的和，$L_{init}$项来自初始视差评估模块和$L_{ref}$项来自视差修正阶段。遵从[12]我们包括了上采样低分辨率预测视差图到全输入分辨率，然后计算相关信号。这个简单的流程强迫逆深度评估在每个尺度上重新生成相同的目标，因此导致更好的输出。特别的，我们得到最终训练损失为  

$$L_{total} = \sum_{s=1}^{n_i} L_{init} + \sum_{s=1}^{nr} L_{ref}$$  

　　此处s表示输出分辨率，$n_i$和$n_r$为计算损失中考虑尺度的数量，同时$L_{init}$和$L_{ref}$形式为：  

$$
L_{init} = \alpha_{ap}(L_{ap}^l+L_{ap}^r) + \alpha_{ds}(L_{ds}^l+L_{ds}^r) + \alpha(L_{ps}^l+L_{ps}^r) \\
L_{ref} = \alpha_{ap} L_{ap}^l + \alpha_{ds}L_{ds}^l + \alpha_{ps}L_{ps}^l
$$

　　此处$L_{ap}$为图像重建损失，$L_{ds}$为光滑项，$L_{ps}$为代理监督损失。每一项都包含了初始视差评估器的左组件和右组件，以及微调阶段的左组件。  

　　**图像重建损失** 一个$l_1$损失和相似测量(SSIM)的线性组合编码了重建图像对于原始图像的质量  

$$
L_{ap} = \frac{1}{N} \sum_{i,j} \alpha \frac{1-SSIM(I_{ij},\hat{I}_{ij})}{2} + (1-\alpha) \mid I_{ij} - \hat{I}_{ij} \mid
$$

　　遵从[11]，我们设置$\alpha=0.85$并使用$3 \times 3$核的SSIM。

　　**视差平滑损失** 这个损失鼓励预测视差局部平滑，视差梯度乘以一个图像域的边缘项。  

$$
L_ds = \frac{1}{N} \sum_{ij} \mid \partial_x d_{ij} \mid e^{- \mid \partial_x I_{ij} \mid} + \mid \partial_y d_{ij} \mid e^{- \mid \partial_y I_{ij} \mid}
$$

　　**代理监督损失** 给定卷积立体算法的代理视差图，第4节中细节描述，我们使用反向Huber损失训练网络：  

$$
L_{ps} = \frac{1}{N} \sum_{ij} berHu(d_{ij},d_{ij}^{st},c) \\
berHu(d_{ij},d_{ij}^{st},c) = \begin{cases} \mid d_{ij} - d_{ij}^{st} \qquad if \quad \mid d_{ij} - d_{ij}^{st} \mid \lt c \\ \frac{\mid d_{ij} - d_{ij}^{st} \mid ^2 - c^2}{2c} \qquad otherwise \end{cases}
$$

　　此处$d_{ij}$和$d_{ij}^{st}分别是图像i,j坐标处像素的预测视差和代理声明，c是以$\alpha \max_{i,j} \mid d_{ij} - d_{ij}^{st} \mid$自适应设置，$\alpha=0.2$。  

![](/assets/Learning_Monocular_Depth_Estimation_Infusing_Traditional_Stereo_Knowledge/Figure_3.png)  
图3. SGM计算代理标签示例。给定原图像(a)，网络利用SGM监督以及左右一致性检查(b)，来训练monoResMatch估计最终视差图(c)。本例中没有执行[11]中的后处理。  

## 4. 代理标签提取

　　为了生成准确的代理标签，我们使用流行的SGM算法，一个快速且高效的从修正的立体匹配推测深度的方法，不需训练。我们的实现中，每个像素计算初始匹配损失，视差假设d应用一个$9 \times 7$的普查变换，并在像素串上计算Hamming距离。然后延8个路径扫描优化，按下式调整初始损失体积：  

$$
E(p,d) = C(p,d) + \min_{j \gt 1} [C(p'd),C(p',d+-1)+P1,C(p',d+-q)+P2] - \min_{k \lt D_{max}} (C(p'k))
$$

像素p和假设d的匹配损失为C(p,d)，P1和P2是两个光滑性损失，不鼓励扫描路径上p和之前像素p‘间的差距。最终视差图D对每个参照图像的像素应用赢家通吃得到。尽管SGM生成相当准确的视差标签，异常点可能负面影响深度模型的训练，如Tonioni所提及[49]。当计算损失时，他们应用学习的置信测量来过滤错误标记。不同的，我们运行无学习的左右一致性检查，查看异常点。通过用SGM提取左右视差图$D^L$和$D^R$，对于左右图像，我们应用下列条件来废弃两张图中有不同视差的像素。  

$$
D(p) = \begin{cases} D(p) \qquad if \quad \mid D^L(p) - D^R(p-D^L(p)) \mid \le \epsilon \\ -1 \qquad otherwise \end{cases}
$$

　　左右一致性检查是一个简单的流程，移除很多错误视差赋值，多数在深度不连续附近，不需要[49]中所需的任何训练。因此，我们的代理标签生成过程不依赖真值深度标签。图3显示了一个提取标签的例子，黑像素是通过左右一致性检查过滤掉的像素。尽管有一些点连续，我们能注意到他们不影响训练网络的最后预测，以及我们的方法能在图像左侧的覆盖区域还原准确视差值。  

## 5. 实验结果

　　本节中，我们描述数据集，实现细节，然后给出大量monoResMatch在不同训练/测试上评估，显示我们的方法一致的超越自监督最新水平方法。这个领域的标准，我们获得了单目深度评估技术的表现，遵循Eigen的协议，从KITTI数据集提取数据，使用稀疏雷达测量作为真值进行评估。另外，我们也进行大量简化模型研究，证明代理SGM算法的监督和有效的架构选择，使我们的流程能够以较大差距改进预测深度图的准确度。  


### 5.1 数据集

　　对于所有实验，我们计算标准单目度量：abs，rel，sq，rel，rmse，rmse log表示了误差度量，预测的$delta \lt \tau$的比率，比率和对于真值的逆比率低于阈值$\tau$。两个主要数据集KITTI和CityScapes。  

　　**KITTI** KITTI立体数据集是修正立体匹配的集合，由61个情景收集(包含42382立体帧)，主要对于驾驶情景。图像尺寸$1242 \times 375$像素。一个雷达，加载并修正到左摄像头，用来测量深度信息。  

　　遵从其他工作，我们将数据集分为两个子集，分别为29和32个场景。我们使用第一组的697帧作为测试，第二组的22600多用作训练。我们将数据集称为自身分割。  

![](/assets/Learning_Monocular_Depth_Estimation_Infusing_Traditional_Stereo_Knowledge/Table_1.png)  
表1. 自分割数据集上的简化模型实验，最大深度为80m。除注明外均进行[11]中的后处理。  

　　**CityScapes** CityScapes数据集包括德国的大约50个城市的立体匹配，由车辆在不同天气状况下得到。它包括22973立体图像对，尺寸为$2048 \times 1024$像素。由于多数图像包含引擎盖，大多数反光，因此导致错误估计。在训练中应用随机切割前，我们弃用最低的20%帧。  

### 5.2 实现细节

　　遵从这个了领域的标准约定，我们使用CityScapes后跟KITTI进行训练。我们将这两个训练数据集为Cityscapes(CS)和KITTI(K)。我们使用Tensorflow进行实现，大约42,500,000个参数，多尺度特征提取器0.51M，初始视差阶段41.4M，微调模块0.6M。试验中，我们在CS上预训练monoResMatch，预训练150,000迭代步，batchsize为6，从原始图像调整尺寸到$1024 \times 512$，在其上随机切割$512 \times 256$像素。我们使用Adam优化器，$\beta_1=0.9$，$\beta_2=0.999$，$\epsilon=1e-8$。我们设置初始学习率为1e-4，在100,000和120,000步后手动减半，然后继续训练直到收敛。在第一步初始化流程后，我们执行全部架构微调，在K数据集的22600KITTI原始图像。尤其的，我们以bachsize为6并从调整尺寸到$1280 \times 384$分辨率中抽取$640 \times 192$像素运行300,000步。这个阶段，我们使用学习率1e-4，在180,000及240,000迭代步减半。我们固定不同损失的为$\alpha_{ap}=1$，$\alpha_{ds}=0.1$，$\alpha_{ps}=1$，$n_i=4$，$n_r=3$。如[11]，数据扩充流程在训练中应用于CS和K中的图像来增加网络的强壮性。测试时，我们如[11,44,58]中后处理视差图。不管怎样，我们初步标示，不同于前述方法，如那些如左边界视差边缘的效应被有效的解决了，通过简单的选取SGM升恒的代理视差图上的随机切割，如图3。  

　　代理监督通过[48]中的SGM实现得到，允许我们快速生成左右图像的视差图，对于CS和K数据集。我们使用左右一致检查处理这些输出，减少异常点数量，如4节讨论，使用\epsilon=1。我们在KITTI 2015训练集的200高质量视差图上获得代理生成器的准确性，测量96.1%像素，有视差误差小于3。相比Tonioni，采用本文中可忽略的从99.6%的精度降低。然而，我们不依赖任何如[41]的基于学习的置信评估，因此我们维护标签抽取，与真值分开。由于SGM运行在图像全分辨率而monoResMatch在提取切割前将输入调整到$1280 \times 384$，我们采用缩放因子1280/W到SGM视差图，W是原始图像宽度。一贯的，monoResMatch评估的深度图需要在测试时乘以W/1280。架构以端到端在单个Titan XP GPU上训练，不需任何阶段处理，测试时0.16s每帧推断深度图，处理KITTI分辨率(约$1280 \times 384$与monoResMatch降采样相容)。  

### 5.3 模型简化研究

　　本节中，我们检查1)SGM的代理监督2)不同monoResMatch组件的影响。这些实验的输出，在自身分割集上进行，如表1。  

　　**代理监督损失分析** 我们从零开始训练Godard的monodepth框架，加入代理损失，然后比较原始模型和使用3Net的更有效的方式。我们可以观察到，相比[11]，代理损失能使monodepth模型更准确，更多的，它也超过[44]中的虚拟三目监督，在两者都达到更好的度量，但对于3Net是\delta \lt 1.25。特别的，回顾图3，代理提取与切割组合很好，解决了立体监督中著名的问题，如左侧视差边界问题。补充材料有额外的高质量例子。  

![](/assets/Learning_Monocular_Depth_Estimation_Infusing_Traditional_Stereo_Knowledge/Table_2.png)
表2. KITTI测试数据集上的大量实验，使用Eigen[6]中的分割，最大深度为80m。最后四项包括[11]中的后处理。Ko，Kr，Ko是K的分割，[58]中定义。最好的结果加粗表示。  

　　**组件分析** 图1，我们评估框架的不同设置，通过切除架构特有的关键模块。首先，我们在K上不用代理监督训练monoResMatch，显示我们的架构已经超越[11]。在CS+K上用代理标签训练，我们注意到，没有任何微调模块，架构已经超越Godard的代理监督代理监督ResNet50模型。增加无任何匹配关系编码的视差调整组件，能有小的改进，加入关联层处理真实和合成特征，类似立体匹配，将变更大的度量。最后，[11]中后处理改善所有得分，尽管大的贡献有基于关联的修正模块给出，如比较无匹配编码和无关联层所感觉到的。最后，通过比较4和11行我们也能感觉到的CS预训练的全模型给出的影响。  

### 5.4 自监督框架比较

　　研究monoResMatch架构和代理监督的贡献细节后，比较我们的框架和最新水平自监督方法进行单目深度评估。表2收集了在前述自分割数据集上评估不同模型得到的结果。这个评估中，我们只考虑在任何训练过程中都没有任何真值标签监督进行训练的方法。我们将单目序列监督，双目监督，或两者。大多数方法在CS和K上训练，除了yang利用不同的K分割子集训练。从表中，我们注意到monoResMathc显著超过所有方法。  

　　为了完成利用稠密合成真值进行监督的方法，我们使用KITTI中少量的声明样本运行外的实验，为了更公平比较。表3收集了不同成都监督的这些实验的输出，使用KITTI2015训练集中准确真值标签，或K中不同量的LiDAR测量样本，100,200,500,700，每个设置下运行5,000迭代步。我们指出，在直接与相同量标签图像进行训练的方法进行比较，monoResMatch总能得到更好的得分，没有例外。更进一步，我们用红色标示出每个度量的所有设置中的最佳得分，发现以200-acrt加K中的500样本训练的monoResMatch在所有度量中得到最好的准确度。这个事实说明方法的高效，能超越最新水平用更多监督数据训练的技术(多于30,000立体匹配以及在ImageNet预训练)。利用传统SGM算法而不是深度立体网络用于代理监督，保证更快和更简单的处理训练流程。  

### 5.5 单一视角立体评估的表现

　　最后，我们进一步将monoResMatch与SVS进行比较，两者由同一原理发生。我们在KITTI2015训练集上微调monoResMatch，如表3，提交在线[31]中的立体标准测试。表4比较了monoResMatch和SVS以及[31]评估的其他技术，分别是monodapth和OCV-BM。D1得分表示在图像不同部分视差错误大于真值3%或5%的像素比率，背景、前景或全部。我们可以从表中看出在D1-bg上有一个大于3%的差距，D1-fg上有大于1%，导致合计降低2.72%。这个结果再一次支持了monoResMatch的卓越性，尽管SVS在很多真值合成图像上进行训练。最后，表4描述了KITTI线上标准测试的定性案例。  

![](/assets/Learning_Monocular_Depth_Estimation_Infusing_Traditional_Stereo_Knowledge/Table_3.png)  
表3. Eigen[6]分割上的实验，最大深度为80m。有少量声明样本监督的不同方法的比较。直接比较的最好结果加粗表示，所有分数最好为红色，monoresMatch总是最好。  

![](/assets/Learning_Monocular_Depth_Estimation_Infusing_Traditional_Stereo_Knowledge/Figure_4.png)  
表2. 单目框架的立体评估。从左至右为输入图像，预测深度和与真值见的误差。最后一行为颜色码，用来显示缺陷的严重性(如[35])。  

![](/assets/Learning_Monocular_Depth_Estimation_Infusing_Traditional_Stereo_Knowledge/Table_4.png)  
表2. KITTI2015标准测试数据集上的大量实验。错误大于真值的3%和5%的像素比率。最好的结果加粗表示。  

## 6 结论

　　本文中，我们提出monoResMatch，一个全新的用于单目深度评估的框架。它联合了1)深思熟虑的抉择来类推立体匹配解决单目深度，基于关联的微调模型，2)更强壮的自监督训练，利用传统算法(非学习)生成的代理真值标签。与最新水平模型相反，我们的架构以端到端的方式独立训练。通过大量实验，我们证明了训练时加入代理监督导致更精确的网络，将此耦合到monoResMatch是自监督单目深度评估的最新水平。  

## 致谢

　　我们感谢本研究中Nvidia捐献的Titan XP GPU的支持。  

## 参考文献
